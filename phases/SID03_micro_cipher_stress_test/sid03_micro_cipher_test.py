"""
SID-03: Residual Anomaly / Micro-Cipher Stress Test

FINAL INTERNAL TEST - Shot in the Dark Phase

Goal: After removing ALL known structure (section conditioning, morphology,
positional bias, etc.), determine whether any small subset exhibits additional
formal organization consistent with an encoding process.

Expected outcome: FAILURE (no signal)
"""

import sys
import os
import math
import random
from collections import defaultdict, Counter
from typing import List, Dict, Set, Tuple, Optional
import numpy as np
from scipy import stats as scipy_stats

sys.path.insert(0, r'C:\git\voynich')

# =============================================================================
# CONFIGURATION & THRESHOLDS
# =============================================================================

# Minimum RRS size for analysis
MIN_RRS_SIZE = 50

# Test thresholds
SUBSTITUTION_THRESHOLD = 0.30  # Max entropy ratio for equivalence class
MI_SIGMA_THRESHOLD = 3.0      # Sigma above baseline for MI test
COMPRESSION_THRESHOLD = 0.15  # 15% gain required
GLOBAL_REAPPEAR_THRESHOLD = 3 # Must appear in 3+ distant folios

# Number of randomization trials
N_RANDOMIZATIONS = 1000

# =============================================================================
# DATA LOADING (adapted from SID-01)
# =============================================================================

def load_transcription():
    """Load and parse the interlinear transcription."""
    filepath = r'C:\git\voynich\data\transcriptions\interlinear_full_words.txt'

    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        header = f.readline()
        for line_num, line in enumerate(f, 1):
            parts = line.strip().split('\t')
            if len(parts) >= 4:
                # Filter to H (PRIMARY) transcriber track only
                transcriber = parts[12].strip('"').strip() if len(parts) > 12 else ''
                if transcriber != 'H':
                    continue
                word = parts[0].strip('"')
                folio = parts[2].strip('"')
                section = parts[3].strip('"')

                # Extract additional fields
                try:
                    line_pos = int(parts[11].strip('"')) if len(parts) > 11 else 0
                    is_line_initial = (parts[13].strip('"') == '1') if len(parts) > 13 else False
                except:
                    line_pos = 0
                    is_line_initial = False

                if word and not word.startswith('*'):
                    data.append({
                        'token': word,
                        'folio': folio,
                        'section': section,
                        'line_pos': line_pos,
                        'is_line_initial': is_line_initial,
                        'corpus_pos': len(data)
                    })

    return data


# Grammar patterns (from canonical grammar)
GRAMMAR_PATTERNS = {
    'prefixes': ['qo', 'ch', 'sh', 'da', 'sa', 'ta', 'ok', 'ot', 'ct', 'kc', 'pc', 'fc'],
    'suffixes': ['aiin', 'dy', 'ol', 'or', 'ar', 'ain', 'ey', 'eey', 'edy', 'y'],
    'core_tokens': {
        'qokaiin', 'chey', 'cheey', 'chedy', 'okaiin', 'dar', 'okal', 'saiin',
        'qokal', 'daiin', 'dol', 'ckhey', 'chol', 'dy', 'cheky', 'tedy', 'ol',
        'aiin', 'checkhy', 'otain', 'shey', 'shol', 'shedy', 'shy', 'shor',
        'qo', 'qok', 'ok', 'al', 'or', 'ar', 'dal', 'qokedy', 'okey', 'okeey'
    }
}

# Hazard tokens (from hazards.py)
FORBIDDEN_PAIRS = [
    ('shey', 'aiin'), ('shey', 'al'), ('shey', 'c'), ('chol', 'r'),
    ('chedy', 'ee'), ('chey', 'chedy'), ('l', 'chol'), ('dy', 'aiin'),
    ('dy', 'chey'), ('or', 'dal'), ('ar', 'chol'), ('qo', 'shey'),
    ('qo', 'shy'), ('ok', 'shol'), ('ol', 'shor'), ('dar', 'qokaiin'),
    ('qokaiin', 'qokedy')
]

HAZARD_TOKENS = set()
for a, b in FORBIDDEN_PAIRS:
    HAZARD_TOKENS.add(a)
    HAZARD_TOKENS.add(b)


def is_grammar_token(token: str) -> bool:
    """Check if token matches grammar patterns."""
    t = token.lower()
    if t in GRAMMAR_PATTERNS['core_tokens']:
        return True
    for pfx in GRAMMAR_PATTERNS['prefixes']:
        if t.startswith(pfx):
            return True
    for sfx in GRAMMAR_PATTERNS['suffixes']:
        if t.endswith(sfx):
            return True
    return False


def classify_corpus(data: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
    """Separate corpus into grammar and residue tokens."""
    grammar = []
    residue = []

    for item in data:
        if is_grammar_token(item['token']):
            grammar.append(item)
        else:
            residue.append(item)

    return grammar, residue


def compute_hazard_distances(data: List[Dict]) -> Dict[int, int]:
    """Compute distance to nearest hazard token for each position."""
    hazard_positions = []
    for i, item in enumerate(data):
        if item['token'].lower() in HAZARD_TOKENS:
            hazard_positions.append(i)

    distances = {}
    for i in range(len(data)):
        if hazard_positions:
            distances[i] = min(abs(i - hp) for hp in hazard_positions)
        else:
            distances[i] = float('inf')

    return distances


# =============================================================================
# STRUCTURAL SUBTRACTION
# =============================================================================

def compute_section_baseline(residue: List[Dict]) -> Dict[str, Counter]:
    """Compute expected token frequencies per section."""
    section_tokens = defaultdict(list)
    for item in residue:
        section_tokens[item['section']].append(item['token'].lower())

    return {s: Counter(tokens) for s, tokens in section_tokens.items()}


def compute_morphological_baseline(residue: List[Dict]) -> Tuple[Counter, Counter]:
    """Compute expected prefix/suffix distributions."""
    prefixes = Counter()
    suffixes = Counter()

    for item in residue:
        t = item['token'].lower()
        if len(t) >= 2:
            prefixes[t[:2]] += 1
            suffixes[t[-2:]] += 1

    return prefixes, suffixes


def compute_positional_baseline(residue: List[Dict]) -> Dict[str, Counter]:
    """Compute expected token frequencies by position type."""
    pos_tokens = defaultdict(list)
    for item in residue:
        key = 'line_initial' if item['is_line_initial'] else 'other'
        pos_tokens[key].append(item['token'].lower())

    return {k: Counter(tokens) for k, tokens in pos_tokens.items()}


def compute_ngram_baseline(residue: List[Dict], n: int = 2) -> Counter:
    """Compute character n-gram statistics."""
    ngrams = Counter()
    for item in residue:
        chars = list(item['token'].lower())
        for i in range(len(chars) - n + 1):
            ngrams[tuple(chars[i:i+n])] += 1
    return ngrams


def compute_residue_density_baseline(residue: List[Dict], data: List[Dict]) -> Dict[str, float]:
    """Compute residue density per section."""
    section_totals = Counter(item['section'] for item in data)
    section_residue = Counter(item['section'] for item in residue)

    return {s: section_residue[s] / section_totals[s] if section_totals[s] > 0 else 0.0
            for s in section_totals}


def compute_token_expected_probability(token: str, section: str,
                                       section_baseline: Dict[str, Counter],
                                       prefix_baseline: Counter,
                                       suffix_baseline: Counter,
                                       total_residue: int) -> float:
    """
    Compute expected probability of token given all baseline models.
    Returns combined probability from section + morphology models.
    """
    t = token.lower()

    # Section-based probability
    sec_counter = section_baseline.get(section, Counter())
    sec_total = sum(sec_counter.values())
    p_section = sec_counter.get(t, 0) / sec_total if sec_total > 0 else 0

    # Morphology-based probability (prefix * suffix independent)
    pfx_total = sum(prefix_baseline.values())
    sfx_total = sum(suffix_baseline.values())

    if len(t) >= 2:
        p_prefix = prefix_baseline.get(t[:2], 0) / pfx_total if pfx_total > 0 else 0
        p_suffix = suffix_baseline.get(t[-2:], 0) / sfx_total if sfx_total > 0 else 0
        p_morph = math.sqrt(p_prefix * p_suffix)  # Geometric mean
    else:
        p_morph = 1.0 / total_residue  # Uniform for short tokens

    # Combined probability (weighted average)
    p_combined = 0.7 * p_section + 0.3 * p_morph

    return max(p_combined, 1e-10)  # Avoid zero


def compute_token_surprisal(residue: List[Dict], section_baseline: Dict[str, Counter],
                            prefix_baseline: Counter, suffix_baseline: Counter) -> List[Tuple[Dict, float]]:
    """
    Compute surprisal (negative log probability) for each token.
    High surprisal = token is unexpected given all baselines.
    """
    total = len(residue)
    surprisals = []

    for item in residue:
        p = compute_token_expected_probability(
            item['token'], item['section'],
            section_baseline, prefix_baseline, suffix_baseline, total
        )
        surprisal = -math.log2(p)
        surprisals.append((item, surprisal))

    return surprisals


def compute_residual_remainder_set(residue: List[Dict],
                                   section_baseline: Dict[str, Counter],
                                   prefix_baseline: Counter,
                                   suffix_baseline: Counter,
                                   surprisal_threshold: float = None) -> List[Dict]:
    """
    Compute the Residual Remainder Set (RRS).

    RRS = tokens with surprisal above threshold (unexplained by baselines).
    If threshold not specified, use top 10% by surprisal.
    """
    surprisals = compute_token_surprisal(residue, section_baseline,
                                          prefix_baseline, suffix_baseline)

    # Sort by surprisal descending
    surprisals.sort(key=lambda x: x[1], reverse=True)

    if surprisal_threshold is None:
        # Use top 10% by surprisal
        threshold_idx = int(len(surprisals) * 0.10)
        if threshold_idx > 0:
            surprisal_threshold = surprisals[threshold_idx - 1][1]
        else:
            surprisal_threshold = surprisals[0][1] if surprisals else 0

    rrs = [item for item, surp in surprisals if surp >= surprisal_threshold]

    return rrs, surprisal_threshold


# =============================================================================
# SUBSET SAMPLING STRATEGIES
# =============================================================================

def sample_s1_rarity_subsets(residue: List[Dict]) -> Dict[str, List[Dict]]:
    """S-1: Rarity-based subsets."""
    subsets = {}

    # Count token frequencies
    token_counts = Counter(item['token'].lower() for item in residue)
    total_types = len(token_counts)

    # Lowest 1%, 2%, 5% frequency tokens
    sorted_tokens = token_counts.most_common()
    sorted_tokens.reverse()  # Ascending frequency

    for pct in [1, 2, 5]:
        n = max(1, int(total_types * pct / 100))
        rare_tokens = set(t for t, _ in sorted_tokens[:n])
        subsets[f'rare_{pct}pct'] = [item for item in residue
                                      if item['token'].lower() in rare_tokens]

    # Tokens appearing in exactly 2-5 locations
    for freq in [2, 3, 4, 5]:
        exact_tokens = set(t for t, c in token_counts.items() if c == freq)
        subsets[f'exact_{freq}_occurrences'] = [item for item in residue
                                                 if item['token'].lower() in exact_tokens]

    # Hapax-like at structured positions (line-initial + folio boundaries)
    hapax = set(t for t, c in token_counts.items() if c <= 2)
    structured_hapax = [item for item in residue
                        if item['token'].lower() in hapax and
                        (item['is_line_initial'] or item['line_pos'] <= 1)]
    subsets['hapax_structured'] = structured_hapax

    return subsets


def sample_s2_positional_anomaly_subsets(residue: List[Dict],
                                          hazard_distances: Dict[int, int]) -> Dict[str, List[Dict]]:
    """S-2: Positional anomaly subsets."""
    subsets = {}

    # Tokens maximally distant from hazards (top 10%)
    with_dist = [(item, hazard_distances.get(item['corpus_pos'], 0)) for item in residue]
    with_dist.sort(key=lambda x: x[1], reverse=True)
    n = max(1, len(with_dist) // 10)
    subsets['max_hazard_distance'] = [item for item, _ in with_dist[:n]]

    # Tokens at extreme line/folio boundaries
    extreme_positions = [item for item in residue if item['line_pos'] == 0 or item['line_pos'] >= 20]
    subsets['extreme_positions'] = extreme_positions

    # Tokens in sections with atypical layout (A, Z have unusual structure)
    atypical_sections = {'A', 'Z'}
    subsets['atypical_section_layout'] = [item for item in residue
                                           if item['section'] in atypical_sections]

    return subsets


def sample_s3_morphological_outliers(residue: List[Dict],
                                      prefix_baseline: Counter,
                                      suffix_baseline: Counter) -> Dict[str, List[Dict]]:
    """S-3: Morphological outlier subsets."""
    subsets = {}

    # Compute expected prefix/suffix entropy
    def entropy(counter):
        total = sum(counter.values())
        if total == 0:
            return 0
        return -sum((c/total) * math.log2(c/total) for c in counter.values() if c > 0)

    pfx_entropy = entropy(prefix_baseline)
    sfx_entropy = entropy(suffix_baseline)

    pfx_total = sum(prefix_baseline.values())
    sfx_total = sum(suffix_baseline.values())

    # For each token, compute prefix/suffix rarity
    outliers_prefix = []
    outliers_suffix = []
    outliers_combined = []

    for item in residue:
        t = item['token'].lower()
        if len(t) >= 2:
            pfx_count = prefix_baseline.get(t[:2], 0)
            sfx_count = suffix_baseline.get(t[-2:], 0)

            # Expected vs observed
            pfx_freq = pfx_count / pfx_total if pfx_total > 0 else 0
            sfx_freq = sfx_count / sfx_total if sfx_total > 0 else 0

            # Z-score approximation (rare = low frequency = high deviation)
            pfx_z = -math.log2(pfx_freq + 1e-10) / pfx_entropy if pfx_entropy > 0 else 0
            sfx_z = -math.log2(sfx_freq + 1e-10) / sfx_entropy if sfx_entropy > 0 else 0

            if pfx_z > 2:
                outliers_prefix.append(item)
            if sfx_z > 2:
                outliers_suffix.append(item)
            if pfx_z > 2 and sfx_z > 2:
                outliers_combined.append(item)

    subsets['prefix_outliers'] = outliers_prefix
    subsets['suffix_outliers'] = outliers_suffix
    subsets['combined_outliers'] = outliers_combined

    # Rare shape combinations (unusual glyph sequences)
    rare_shapes = []
    for item in residue:
        t = item['token'].lower()
        # Check for unusual character combinations
        if any(c in t for c in ['x', 'z', 'j', 'v', 'w', 'b', 'u']):
            rare_shapes.append(item)
    subsets['rare_shapes'] = rare_shapes

    return subsets


def sample_s4_cooccurrence_cliques(residue: List[Dict]) -> Dict[str, List[Dict]]:
    """S-4: Co-occurrence clique subsets."""
    subsets = {}

    # Build adjacency counts
    adjacent_pairs = Counter()
    for i in range(len(residue) - 1):
        t1 = residue[i]['token'].lower()
        t2 = residue[i+1]['token'].lower()
        if t1 != t2:
            pair = tuple(sorted([t1, t2]))
            adjacent_pairs[pair] += 1

    # Find pairs with unusually high co-occurrence
    # (compared to product of individual frequencies)
    token_counts = Counter(item['token'].lower() for item in residue)
    total_adj = sum(adjacent_pairs.values())
    total_tokens = len(residue)

    unusual_pairs = []
    for (t1, t2), count in adjacent_pairs.items():
        if count >= 3:  # At least 3 co-occurrences
            expected = (token_counts[t1] * token_counts[t2]) / (total_tokens ** 2) * total_adj
            if expected > 0 and count / expected > 3:  # 3x more than expected
                unusual_pairs.append((t1, t2, count, count/expected))

    # Collect tokens from unusual pairs
    clique_tokens = set()
    for t1, t2, _, _ in unusual_pairs:
        clique_tokens.add(t1)
        clique_tokens.add(t2)

    subsets['cooccurrence_cliques'] = [item for item in residue
                                        if item['token'].lower() in clique_tokens]

    return subsets


def sample_s5_stability_subsets(residue: List[Dict]) -> Dict[str, List[Dict]]:
    """S-5: Stability-across-sections subsets (despite 82% section exclusivity)."""
    subsets = {}

    # Find tokens appearing in multiple sections
    token_sections = defaultdict(set)
    token_counts = Counter()

    for item in residue:
        t = item['token'].lower()
        token_sections[t].add(item['section'])
        token_counts[t] += 1

    # Require: >=3 sections, <=30 total occurrences
    cross_section_tokens = set()
    for t, sections in token_sections.items():
        if len(sections) >= 3 and token_counts[t] <= 30:
            cross_section_tokens.add(t)

    subsets['cross_section_stable'] = [item for item in residue
                                        if item['token'].lower() in cross_section_tokens]

    # Tokens with exactly the same folio distribution (appear in same folios)
    token_folios = defaultdict(set)
    for item in residue:
        token_folios[item['token'].lower()].add(item['folio'])

    # Group by folio signature
    folio_sig_groups = defaultdict(list)
    for t, folios in token_folios.items():
        if len(folios) >= 2:  # At least 2 folios
            sig = tuple(sorted(folios))
            folio_sig_groups[sig].append(t)

    # Find groups with multiple tokens sharing exact folio pattern
    matched_folios_tokens = set()
    for sig, tokens in folio_sig_groups.items():
        if len(tokens) >= 2 and len(sig) >= 3:
            matched_folios_tokens.update(tokens)

    subsets['matched_folio_pattern'] = [item for item in residue
                                         if item['token'].lower() in matched_folios_tokens]

    return subsets


# =============================================================================
# FORMAL TESTS
# =============================================================================

def test_a_substitution_invariance(subset: List[Dict]) -> Tuple[bool, Dict]:
    """
    TEST A: Substitution Invariance

    Check if tokens form stable equivalence classes with bijective remapping.
    """
    if len(subset) < 10:
        return False, {'reason': 'subset too small', 'n': len(subset)}

    tokens = [item['token'].lower() for item in subset]
    token_set = set(tokens)

    if len(token_set) < 3:
        return False, {'reason': 'insufficient vocabulary', 'types': len(token_set)}

    # Build context distribution for each token
    context_signatures = {}
    for i, item in enumerate(subset):
        t = item['token'].lower()
        left = subset[i-1]['token'].lower() if i > 0 else '<START>'
        right = subset[i+1]['token'].lower() if i < len(subset)-1 else '<END>'

        if t not in context_signatures:
            context_signatures[t] = Counter()
        context_signatures[t][(left, right)] += 1

    # Check if tokens cluster into distinct equivalence classes
    # by comparing context distributions
    token_list = list(context_signatures.keys())
    n_tokens = len(token_list)

    if n_tokens < 3:
        return False, {'reason': 'insufficient token types', 'types': n_tokens}

    # Compute pairwise distribution similarity (Jensen-Shannon divergence)
    similarities = []
    for i in range(n_tokens):
        for j in range(i+1, n_tokens):
            t1, t2 = token_list[i], token_list[j]
            c1, c2 = context_signatures[t1], context_signatures[t2]

            # Union of contexts
            all_contexts = set(c1.keys()) | set(c2.keys())

            # Normalize to distributions
            total1 = sum(c1.values()) + 1e-10
            total2 = sum(c2.values()) + 1e-10

            js = 0
            for ctx in all_contexts:
                p1 = c1.get(ctx, 0) / total1
                p2 = c2.get(ctx, 0) / total2
                m = (p1 + p2) / 2
                if p1 > 0:
                    js += p1 * math.log2(p1 / m) / 2
                if p2 > 0:
                    js += p2 * math.log2(p2 / m) / 2

            similarities.append((t1, t2, js))

    # Check for clustering: do some pairs have very low divergence (equivalence)?
    js_values = [s for _, _, s in similarities]
    if not js_values:
        return False, {'reason': 'no pairs to compare'}

    mean_js = np.mean(js_values)
    std_js = np.std(js_values)

    # Look for pairs with JS < mean - 2*std (strong equivalence)
    equivalence_threshold = mean_js - 2 * std_js if std_js > 0 else mean_js * 0.5
    equivalent_pairs = [(t1, t2) for t1, t2, js in similarities if js < equivalence_threshold]

    # Build equivalence classes
    from collections import deque

    token_to_class = {}
    classes = []

    for t1, t2 in equivalent_pairs:
        if t1 in token_to_class and t2 in token_to_class:
            # Merge classes
            c1, c2 = token_to_class[t1], token_to_class[t2]
            if c1 != c2:
                classes[c1].update(classes[c2])
                for t in classes[c2]:
                    token_to_class[t] = c1
                classes[c2] = set()
        elif t1 in token_to_class:
            classes[token_to_class[t1]].add(t2)
            token_to_class[t2] = token_to_class[t1]
        elif t2 in token_to_class:
            classes[token_to_class[t2]].add(t1)
            token_to_class[t1] = token_to_class[t2]
        else:
            new_class = len(classes)
            classes.append({t1, t2})
            token_to_class[t1] = new_class
            token_to_class[t2] = new_class

    # Count non-empty classes
    non_empty = [c for c in classes if len(c) > 1]

    result = {
        'n_tokens': len(tokens),
        'n_types': len(token_set),
        'mean_js_divergence': mean_js,
        'std_js_divergence': std_js,
        'equivalence_threshold': equivalence_threshold,
        'n_equivalent_pairs': len(equivalent_pairs),
        'n_equivalence_classes': len(non_empty),
        'class_sizes': [len(c) for c in non_empty]
    }

    # Pass if we found stable equivalence classes
    passes = len(non_empty) >= 2 and mean_js < SUBSTITUTION_THRESHOLD
    return passes, result


def test_b_information_gain(subset: List[Dict], residue: List[Dict]) -> Tuple[bool, Dict]:
    """
    TEST B: Information Gain

    Check if subset has mutual information exceeding random baseline by >=3 sigma.
    """
    if len(subset) < 10:
        return False, {'reason': 'subset too small', 'n': len(subset)}

    tokens = [item['token'].lower() for item in subset]

    # Compute bigram mutual information within subset
    def compute_mi(token_list):
        """Compute mutual information of adjacent token pairs."""
        if len(token_list) < 2:
            return 0.0

        unigrams = Counter(token_list)
        bigrams = Counter(zip(token_list[:-1], token_list[1:]))

        total_uni = sum(unigrams.values())
        total_bi = sum(bigrams.values())

        if total_uni == 0 or total_bi == 0:
            return 0.0

        mi = 0.0
        for (t1, t2), count in bigrams.items():
            p_joint = count / total_bi
            p_t1 = unigrams[t1] / total_uni
            p_t2 = unigrams[t2] / total_uni

            if p_t1 > 0 and p_t2 > 0 and p_joint > 0:
                mi += p_joint * math.log2(p_joint / (p_t1 * p_t2))

        return mi

    observed_mi = compute_mi(tokens)

    # Generate null distribution by shuffling
    null_mis = []
    for _ in range(N_RANDOMIZATIONS):
        shuffled = tokens.copy()
        random.shuffle(shuffled)
        null_mis.append(compute_mi(shuffled))

    null_mean = np.mean(null_mis)
    null_std = np.std(null_mis)

    if null_std > 0:
        z_score = (observed_mi - null_mean) / null_std
    else:
        z_score = 0 if observed_mi == null_mean else float('inf')

    result = {
        'n_tokens': len(tokens),
        'observed_mi': observed_mi,
        'null_mean': null_mean,
        'null_std': null_std,
        'z_score': z_score,
        'threshold': MI_SIGMA_THRESHOLD
    }

    passes = z_score >= MI_SIGMA_THRESHOLD
    return passes, result


def test_c_compression_advantage(subset: List[Dict], residue: List[Dict]) -> Tuple[bool, Dict]:
    """
    TEST C: Compression Advantage

    Check if subset achieves >=15% compression gain beyond morphology-only model.
    """
    if len(subset) < 10:
        return False, {'reason': 'subset too small', 'n': len(subset)}

    tokens = [item['token'].lower() for item in subset]

    # Baseline: entropy of token distribution (morphology-independent)
    token_counts = Counter(tokens)
    total = sum(token_counts.values())

    baseline_entropy = 0
    for count in token_counts.values():
        p = count / total
        baseline_entropy -= p * math.log2(p)

    # Morphology-based compression: use prefix+suffix codebook
    prefixes = Counter()
    suffixes = Counter()
    stems = Counter()

    for t in tokens:
        if len(t) >= 4:
            prefixes[t[:2]] += 1
            suffixes[t[-2:]] += 1
            stems[t[2:-2]] += 1
        elif len(t) >= 2:
            prefixes[t] += 1

    # Compute entropy of morphological components
    def entropy(counter):
        total = sum(counter.values())
        if total == 0:
            return 0
        return -sum((c/total) * math.log2(c/total) for c in counter.values() if c > 0)

    morph_entropy = entropy(prefixes) + entropy(suffixes) + entropy(stems)

    # Dictionary compression: LZ77-style repeated substring counting
    # Simple approximation: count unique substrings
    all_text = ''.join(tokens)
    unique_substrings = set()
    for length in range(2, min(8, len(all_text))):
        for i in range(len(all_text) - length + 1):
            unique_substrings.add(all_text[i:i+length])

    # Compression estimate: unique substrings / total length
    dict_compression = len(unique_substrings) / len(all_text) if all_text else 1.0

    # Compare subset to full residue
    all_residue_tokens = [item['token'].lower() for item in residue]
    all_residue_text = ''.join(all_residue_tokens)

    all_unique = set()
    for length in range(2, min(8, len(all_residue_text))):
        for i in range(len(all_residue_text) - length + 1):
            all_unique.add(all_residue_text[i:i+length])

    full_compression = len(all_unique) / len(all_residue_text) if all_residue_text else 1.0

    # Compression gain: how much better is subset than baseline?
    compression_gain = (baseline_entropy - morph_entropy) / baseline_entropy if baseline_entropy > 0 else 0

    result = {
        'n_tokens': len(tokens),
        'baseline_entropy': baseline_entropy,
        'morph_entropy': morph_entropy,
        'compression_gain': compression_gain,
        'dict_compression_ratio': dict_compression,
        'full_residue_compression': full_compression,
        'threshold': COMPRESSION_THRESHOLD
    }

    passes = compression_gain >= COMPRESSION_THRESHOLD
    return passes, result


def test_d_global_consistency(subset: List[Dict]) -> Tuple[bool, Dict]:
    """
    TEST D: Global Consistency

    Check if patterns reappear across distant folios (not single-section phenomena).
    """
    if len(subset) < 10:
        return False, {'reason': 'subset too small', 'n': len(subset)}

    # Get folio distribution
    token_folios = defaultdict(set)
    token_sections = defaultdict(set)

    for item in subset:
        t = item['token'].lower()
        token_folios[t].add(item['folio'])
        token_sections[t].add(item['section'])

    # Check for tokens appearing in multiple distant folios
    # "Distant" = different sections
    cross_section_tokens = [t for t, secs in token_sections.items() if len(secs) >= 2]
    multi_folio_tokens = [t for t, fols in token_folios.items() if len(fols) >= GLOBAL_REAPPEAR_THRESHOLD]

    # Check for pattern repetition across folios
    folio_tokens = defaultdict(list)
    for item in subset:
        folio_tokens[item['folio']].append(item['token'].lower())

    # Look for common subsequences across folios
    common_subsequences = Counter()
    folios_list = list(folio_tokens.keys())

    for i, f1 in enumerate(folios_list):
        for f2 in folios_list[i+1:]:
            tokens1 = set(folio_tokens[f1])
            tokens2 = set(folio_tokens[f2])
            shared = tokens1 & tokens2
            for t in shared:
                common_subsequences[t] += 1

    n_globally_consistent = len([t for t, c in common_subsequences.items() if c >= 3])

    result = {
        'n_tokens': len(subset),
        'n_types': len(token_folios),
        'n_folios': len(folio_tokens),
        'n_cross_section_tokens': len(cross_section_tokens),
        'n_multi_folio_tokens': len(multi_folio_tokens),
        'n_globally_consistent': n_globally_consistent,
        'threshold': GLOBAL_REAPPEAR_THRESHOLD
    }

    # Pass if we have globally consistent patterns
    passes = n_globally_consistent >= 3 or len(cross_section_tokens) >= 5
    return passes, result


# =============================================================================
# MAIN ANALYSIS
# =============================================================================

def run_sid03():
    """Execute SID-03 Residual Anomaly / Micro-Cipher Stress Test."""

    print("="*70)
    print("SID-03: RESIDUAL ANOMALY / MICRO-CIPHER STRESS TEST")
    print("="*70)
    print()
    print("FINAL INTERNAL TEST - Expected outcome: NO SIGNAL")
    print()

    # Set seeds for reproducibility
    random.seed(42)
    np.random.seed(42)

    # ==========================================================================
    # LOAD DATA
    # ==========================================================================

    print("Loading corpus...")
    data = load_transcription()
    print(f"  Total corpus: {len(data)} tokens")

    grammar, residue = classify_corpus(data)
    print(f"  Grammar tokens: {len(grammar)} ({len(grammar)/len(data)*100:.1f}%)")
    print(f"  Residue tokens: {len(residue)} ({len(residue)/len(data)*100:.1f}%)")
    print()

    # ==========================================================================
    # SECTION A: STRUCTURE SUBTRACTION SUMMARY
    # ==========================================================================

    print("="*70)
    print("SECTION A: STRUCTURE SUBTRACTION SUMMARY")
    print("="*70)
    print()

    # Compute all baselines
    print("Computing structural baselines...")

    section_baseline = compute_section_baseline(residue)
    prefix_baseline, suffix_baseline = compute_morphological_baseline(residue)
    positional_baseline = compute_positional_baseline(residue)
    bigram_baseline = compute_ngram_baseline(residue, 2)
    density_baseline = compute_residue_density_baseline(residue, data)
    hazard_distances = compute_hazard_distances(data)

    print()
    print("Structural models subtracted:")
    print()
    print("| Model | Parameters | Coverage |")
    print("|-------|------------|----------|")
    print(f"| Section conditioning | {len(section_baseline)} sections | 82% exclusivity |")
    print(f"| Morphological (prefix) | {len(prefix_baseline)} patterns | - |")
    print(f"| Morphological (suffix) | {len(suffix_baseline)} patterns | - |")
    print(f"| Positional bias | {len(positional_baseline)} position types | - |")
    print(f"| N-gram statistics | {len(bigram_baseline)} bigrams | - |")
    print(f"| Residue density | {len(density_baseline)} sections | - |")
    print()

    # ==========================================================================
    # SECTION B: RESIDUAL REMAINDER SET
    # ==========================================================================

    print("="*70)
    print("SECTION B: RESIDUAL REMAINDER SET (RRS)")
    print("="*70)
    print()

    rrs, threshold = compute_residual_remainder_set(
        residue, section_baseline, prefix_baseline, suffix_baseline
    )

    print(f"Surprisal threshold: {threshold:.2f} bits")
    print(f"RRS size: {len(rrs)} tokens")
    print(f"RRS types: {len(set(item['token'].lower() for item in rrs))}")
    print()

    # Check minimum size
    if len(rrs) < MIN_RRS_SIZE:
        print(f"â›” IMMEDIATE STOP: |RRS| = {len(rrs)} < {MIN_RRS_SIZE}")
        print()
        print("NO SIGNAL POSSIBLE - Residual remainder too small for analysis.")
        print()
        return "STOP", None

    # RRS statistics
    rrs_sections = Counter(item['section'] for item in rrs)
    rrs_folios = Counter(item['folio'] for item in rrs)
    rrs_line_initial = sum(1 for item in rrs if item['is_line_initial'])

    print("RRS distribution:")
    print(f"  Sections: {dict(rrs_sections)}")
    print(f"  Folios: {len(rrs_folios)} unique")
    print(f"  Line-initial: {rrs_line_initial} ({rrs_line_initial/len(rrs)*100:.1f}%)")
    print()

    # ==========================================================================
    # SECTION C: SUBSET SAMPLING MATRIX
    # ==========================================================================

    print("="*70)
    print("SECTION C: SUBSET SAMPLING MATRIX")
    print("="*70)
    print()

    # Sample all subsets
    subsets = {}

    print("S-1: Rarity-based subsets...")
    s1 = sample_s1_rarity_subsets(residue)
    subsets.update(s1)

    print("S-2: Positional anomaly subsets...")
    s2 = sample_s2_positional_anomaly_subsets(residue, hazard_distances)
    subsets.update(s2)

    print("S-3: Morphological outlier subsets...")
    s3 = sample_s3_morphological_outliers(residue, prefix_baseline, suffix_baseline)
    subsets.update(s3)

    print("S-4: Co-occurrence clique subsets...")
    s4 = sample_s4_cooccurrence_cliques(residue)
    subsets.update(s4)

    print("S-5: Stability-across-sections subsets...")
    s5 = sample_s5_stability_subsets(residue)
    subsets.update(s5)

    print()
    print("| Strategy | Subset Name | Size | Types |")
    print("|----------|-------------|------|-------|")
    for name, items in sorted(subsets.items()):
        types = len(set(item['token'].lower() for item in items))
        strategy = name.split('_')[0] if '_' in name else name[:4]
        print(f"| S-{strategy} | {name} | {len(items)} | {types} |")
    print()

    # ==========================================================================
    # SECTION D: TEST RESULTS TABLE
    # ==========================================================================

    print("="*70)
    print("SECTION D: TEST RESULTS TABLE")
    print("="*70)
    print()

    results = {}

    print("Running formal tests on all subsets...")
    print()

    for name, subset in subsets.items():
        if len(subset) < 10:
            results[name] = {
                'A': (False, {'reason': 'too small'}),
                'B': (False, {'reason': 'too small'}),
                'C': (False, {'reason': 'too small'}),
                'D': (False, {'reason': 'too small'})
            }
            continue

        results[name] = {
            'A': test_a_substitution_invariance(subset),
            'B': test_b_information_gain(subset, residue),
            'C': test_c_compression_advantage(subset, residue),
            'D': test_d_global_consistency(subset)
        }

    # Print results table
    print("| Subset | Test A | Test B | Test C | Test D | ALL PASS |")
    print("|--------|--------|--------|--------|--------|----------|")

    any_all_pass = False
    passing_subsets = []

    for name, tests in sorted(results.items()):
        a_pass, a_data = tests['A']
        b_pass, b_data = tests['B']
        c_pass, c_data = tests['C']
        d_pass, d_data = tests['D']

        all_pass = a_pass and b_pass and c_pass and d_pass
        if all_pass:
            any_all_pass = True
            passing_subsets.append(name)

        a_str = "PASS" if a_pass else "FAIL"
        b_str = "PASS" if b_pass else "FAIL"
        c_str = "PASS" if c_pass else "FAIL"
        d_str = "PASS" if d_pass else "FAIL"
        all_str = "[OK] YES" if all_pass else "[XX] NO"

        print(f"| {name[:20]:<20} | {a_str} | {b_str} | {c_str} | {d_str} | {all_str} |")

    print()

    # ==========================================================================
    # SECTION E: SURVIVING CANDIDATES
    # ==========================================================================

    print("="*70)
    print("SECTION E: SURVIVING CANDIDATES")
    print("="*70)
    print()

    if any_all_pass:
        print(f"Candidates passing ALL tests: {len(passing_subsets)}")
        print()
        for name in passing_subsets:
            subset = subsets[name]
            tests = results[name]

            print(f"CANDIDATE: {name}")
            print(f"  Size: {len(subset)} tokens")
            print(f"  Types: {len(set(item['token'].lower() for item in subset))}")

            # Show test details
            for test_name, (passed, data) in tests.items():
                print(f"  Test {test_name}: {data}")
            print()

        # Resample to verify stability
        print("STABILITY VERIFICATION (resampling)...")

        stable_candidates = []
        for name in passing_subsets:
            subset = subsets[name]

            # Run tests on 5 bootstrap samples
            bootstrap_passes = []
            for _ in range(5):
                sample = random.choices(subset, k=len(subset))
                a, _ = test_a_substitution_invariance(sample)
                b, _ = test_b_information_gain(sample, residue)
                c, _ = test_c_compression_advantage(sample, residue)
                d, _ = test_d_global_consistency(sample)
                bootstrap_passes.append(a and b and c and d)

            stability = sum(bootstrap_passes) / len(bootstrap_passes)
            print(f"  {name}: stability = {stability*100:.0f}%")

            if stability >= 0.6:  # Passes in >60% of bootstraps
                stable_candidates.append(name)

        print()

        if stable_candidates:
            print(f"STABLE CANDIDATES: {len(stable_candidates)}")
            for name in stable_candidates:
                print(f"  - {name}")
        else:
            print("No candidates survive bootstrap resampling.")
            print()
            print("**NONE** - All candidates collapse under resampling.")
    else:
        print("**NONE**")
        print()
        print("No subset passed all four tests.")

    # ==========================================================================
    # SECTION F: VERDICT
    # ==========================================================================

    print()
    print("="*70)
    print("SECTION F: VERDICT")
    print("="*70)
    print()

    if any_all_pass and 'stable_candidates' in dir() and stable_candidates:
        verdict = "SIGNAL_DETECTED"
        print("+-----------------------------------------------------------------------+")
        print("|                                                                       |")
        print("|   [OK] NON-TRIVIAL MICRO-STRUCTURE DETECTED                          |")
        print("|                                                                       |")
        print("|   Stable candidates survive all tests including resampling.          |")
        print("|   This does NOT imply cipher or meaning.                             |")
        print("|   Formal description only.                                           |")
        print("|                                                                       |")
        print("+-----------------------------------------------------------------------+")
    else:
        verdict = "NO_SIGNAL"
        print("+-----------------------------------------------------------------------+")
        print("|                                                                       |")
        print("|   [XX] NO ADDITIONAL STRUCTURE REMAINS                               |")
        print("|                                                                       |")
        print("|   After subtracting section conditioning, morphology, positional     |")
        print("|   bias, and n-gram statistics, no micro-subset exhibits formal       |")
        print("|   organization exceeding noise baselines.                            |")
        print("|                                                                       |")
        print("|   All tested subsets either:                                         |")
        print("|     - Failed at least one formal test, OR                            |")
        print("|     - Collapsed under bootstrap resampling                           |")
        print("|                                                                       |")
        print("+-----------------------------------------------------------------------+")

    print()

    # ==========================================================================
    # SECTION G: PROJECT STATUS UPDATE
    # ==========================================================================

    print("="*70)
    print("SECTION G: PROJECT STATUS UPDATE")
    print("="*70)
    print()

    if verdict == "NO_SIGNAL":
        print("INTERNAL INVESTIGATION STATUS: EXHAUSTED")
        print()
        print("SID-03 confirms that after removing all documented structure:")
        print("  - Section-level conditioning (82% exclusivity)")
        print("  - Morphological patterns (prefix/suffix)")
        print("  - Positional bias (line-initial, folio boundaries)")
        print("  - Character n-gram statistics")
        print("  - Residue density variation")
        print()
        print("No residual micro-structure remains that could indicate:")
        print("  - Hidden cipher layers")
        print("  - Additional encoding schemes")
        print("  - Anomalous formal organization")
        print()
        print("The residue is FULLY EXPLAINED by the documented structural models.")
        print()
        print("+-----------------------------------------------------------------------+")
        print("|                                                                       |")
        print("|   FURTHER INTERNAL TESTS ARE NOT JUSTIFIED                           |")
        print("|                                                                       |")
        print("|   The internal structure of the Voynich Manuscript has been          |")
        print("|   exhaustively characterized. No additional internal analysis        |")
        print("|   can yield new structural findings.                                 |")
        print("|                                                                       |")
        print("|   Any future progress requires:                                      |")
        print("|     - New external evidence (historical, codicological)              |")
        print("|     - Physical analysis (radiocarbon, pigment, etc.)                 |")
        print("|     - Archival discoveries                                           |")
        print("|                                                                       |")
        print("|   Internal text analysis is COMPLETE.                                |")
        print("|                                                                       |")
        print("+-----------------------------------------------------------------------+")
    else:
        print("INTERNAL INVESTIGATION STATUS: OPEN")
        print()
        print("SID-03 detected potential micro-structure that survives all tests.")
        print("Further investigation of stable candidates MAY be justified.")
        print()
        print("However, detection of structure does NOT imply:")
        print("  - Cipher or encoding")
        print("  - Semantic meaning")
        print("  - Intentional design")
        print()
        print("Additional falsification tests required before drawing conclusions.")

    print()
    print("="*70)
    print("SID-03 COMPLETE")
    print("="*70)

    return verdict, results


if __name__ == "__main__":
    verdict, results = run_sid03()
