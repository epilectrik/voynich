#!/usr/bin/env python3
"""
T6: State Transition Topology â€” Directed Graph Analysis
MINIMAL_STATE_AUTOMATON phase

Analyze the 6-state transition matrix as a directed graph:
- Edge classification (strong/moderate/weak/forbidden)
- Self-loop rates and trapping tendency
- Net flow asymmetry between states
- Steady-state distribution
- Mixing time and communicability
- Control-loop arc identification
"""

import sys
import json
import functools
import numpy as np
from pathlib import Path

sys.stdout.reconfigure(encoding='utf-8')
print = functools.partial(print, flush=True)

RESULTS_DIR = Path(__file__).parent.parent / 'results'

STATE_LABELS = [
    'S0: FL_HAZ',
    'S1: FQ',
    'S2: CC',
    'S3: AX/EN minor',
    'S4: AX/EN major',
    'S5: FL_SAFE',
]

SHORT_LABELS = ['FL_H', 'FQ', 'CC', 'AXm', 'AXM', 'FL_S']


def run():
    print("=" * 70)
    print("T6: State Transition Topology")
    print("MINIMAL_STATE_AUTOMATON phase")
    print("=" * 70)

    # Load data
    with open(RESULTS_DIR / 't4_synthetic.json') as f:
        t4 = json.load(f)
    with open(RESULTS_DIR / 't3_merged_automaton.json') as f:
        t3 = json.load(f)

    P = np.array(t4['state_transition_matrix'])
    n = len(P)

    # =========================================================
    # 1. Edge Classification
    # =========================================================
    print("\n[1/6] Edge Classification")
    print(f"  Transition matrix ({n}x{n}):")
    print(f"  {'':>6}", end='')
    for j in range(n):
        print(f"  {SHORT_LABELS[j]:>5}", end='')
    print()
    for i in range(n):
        print(f"  {SHORT_LABELS[i]:>5}", end='')
        for j in range(n):
            print(f"  {P[i][j]:5.3f}", end='')
        print()

    strong = []     # > 0.10
    moderate = []   # 0.03 - 0.10
    weak = []       # 0.01 - 0.03
    negligible = [] # < 0.01

    for i in range(n):
        for j in range(n):
            edge = (i, j, P[i][j])
            if P[i][j] > 0.10:
                strong.append(edge)
            elif P[i][j] >= 0.03:
                moderate.append(edge)
            elif P[i][j] >= 0.01:
                weak.append(edge)
            else:
                negligible.append(edge)

    print(f"\n  Strong edges (>0.10): {len(strong)}")
    for i, j, p in sorted(strong, key=lambda x: -x[2]):
        print(f"    {SHORT_LABELS[i]:>4} -> {SHORT_LABELS[j]:<4}: {p:.3f}")

    print(f"\n  Moderate edges (0.03-0.10): {len(moderate)}")
    for i, j, p in sorted(moderate, key=lambda x: -x[2]):
        print(f"    {SHORT_LABELS[i]:>4} -> {SHORT_LABELS[j]:<4}: {p:.3f}")

    print(f"\n  Weak edges (0.01-0.03): {len(weak)}")
    for i, j, p in sorted(weak, key=lambda x: -x[2]):
        print(f"    {SHORT_LABELS[i]:>4} -> {SHORT_LABELS[j]:<4}: {p:.3f}")

    print(f"\n  Negligible edges (<0.01): {len(negligible)}")
    for i, j, p in sorted(negligible, key=lambda x: -x[2]):
        print(f"    {SHORT_LABELS[i]:>4} -> {SHORT_LABELS[j]:<4}: {p:.3f}")

    # =========================================================
    # 2. Self-Loop Analysis
    # =========================================================
    print("\n[2/6] Self-Loop Analysis")
    diag = np.diag(P)
    for i in range(n):
        mean_exit = np.mean([P[i][j] for j in range(n) if j != i])
        print(f"  {STATE_LABELS[i]:>20}: self-loop={diag[i]:.3f}  "
              f"mean_exit={mean_exit:.3f}  "
              f"expected_dwell={1/(1-diag[i]):.1f} tokens")

    # =========================================================
    # 3. Net Flow Asymmetry
    # =========================================================
    print("\n[3/6] Net Flow Asymmetry (P[i->j] vs P[j->i])")
    # Weight by stationary distribution
    state_freqs = np.array([sd['frequency_fraction'] for sd in t3['state_descriptions']])

    asymmetric_pairs = []
    for i in range(n):
        for j in range(i+1, n):
            flow_ij = state_freqs[i] * P[i][j]  # probability mass flowing i->j
            flow_ji = state_freqs[j] * P[j][i]  # probability mass flowing j->i
            net = flow_ij - flow_ji
            ratio = P[i][j] / P[j][i] if P[j][i] > 0 else float('inf')
            asymmetric_pairs.append((i, j, P[i][j], P[j][i], ratio, net))

    # Sort by absolute net flow
    for i, j, pij, pji, ratio, net in sorted(asymmetric_pairs, key=lambda x: -abs(x[5])):
        direction = "->" if net > 0 else "<-"
        print(f"  {SHORT_LABELS[i]:>4} <-> {SHORT_LABELS[j]:<4}: "
              f"P[{SHORT_LABELS[i]}->{SHORT_LABELS[j]}]={pij:.3f}  "
              f"P[{SHORT_LABELS[j]}->{SHORT_LABELS[i]}]={pji:.3f}  "
              f"ratio={ratio:.2f}x  net_flow={direction} {abs(net):.4f}")

    # =========================================================
    # 4. Steady-State Distribution
    # =========================================================
    print("\n[4/6] Steady-State Distribution")
    # Compute from eigendecomposition
    eigenvalues, eigenvectors = np.linalg.eig(P.T)
    # Find eigenvector for eigenvalue closest to 1
    idx = np.argmin(np.abs(eigenvalues - 1.0))
    stationary = np.real(eigenvectors[:, idx])
    stationary = stationary / stationary.sum()

    print(f"  From eigendecomposition:")
    for i in range(n):
        print(f"    {STATE_LABELS[i]:>20}: pi={stationary[i]:.4f}  "
              f"empirical={state_freqs[i]:.4f}  "
              f"diff={stationary[i]-state_freqs[i]:+.4f}")

    # =========================================================
    # 5. Mixing Properties
    # =========================================================
    print("\n[5/6] Mixing Properties")
    # Eigenvalues of P
    evals = np.linalg.eigvals(P)
    evals_sorted = sorted(evals, key=lambda x: -abs(x))
    print(f"  Eigenvalue magnitudes:")
    for k, ev in enumerate(evals_sorted):
        print(f"    lambda_{k}: |{abs(ev):.6f}|  (real={ev.real:+.6f}, imag={ev.imag:+.6f})")

    # Spectral gap = 1 - |lambda_2|
    spectral_gap = 1 - abs(evals_sorted[1])
    mixing_time = 1 / spectral_gap if spectral_gap > 0 else float('inf')
    print(f"\n  Spectral gap: {spectral_gap:.4f}")
    print(f"  Mixing time (1/gap): {mixing_time:.1f} tokens")
    print(f"  Interpretation: After ~{int(mixing_time)+1} tokens, initial state is 'forgotten'")

    # Power iteration: how fast does P^k converge?
    print(f"\n  Convergence profile (TV distance from stationary):")
    pi_row = stationary.reshape(1, -1)
    Pk = np.eye(n)
    for k in [1, 2, 3, 5, 10, 20]:
        Pk = np.linalg.matrix_power(P, k)
        max_tv = max(0.5 * np.sum(np.abs(Pk[i] - stationary)) for i in range(n))
        print(f"    k={k:>2}: max_TV = {max_tv:.4f}")

    # =========================================================
    # 6. Control Arc Analysis
    # =========================================================
    print("\n[6/6] Control Arc Analysis")
    # Identify dominant flow patterns
    # For each state, what's its primary destination?
    print(f"\n  Primary destinations (excluding self-loops):")
    for i in range(n):
        exits = [(j, P[i][j]) for j in range(n) if j != i]
        exits.sort(key=lambda x: -x[1])
        top = exits[0]
        second = exits[1] if len(exits) > 1 else None
        line = f"  {STATE_LABELS[i]:>20} -> {SHORT_LABELS[top[0]]} ({top[1]:.3f})"
        if second and second[1] > 0.03:
            line += f", {SHORT_LABELS[second[0]]} ({second[1]:.3f})"
        print(line)

    # Primary sources (who feeds each state?)
    print(f"\n  Primary sources (weighted by mass):")
    for j in range(n):
        sources = [(i, state_freqs[i] * P[i][j]) for i in range(n) if i != j]
        sources.sort(key=lambda x: -x[1])
        top = sources[0]
        second = sources[1] if len(sources) > 1 else None
        line = f"  {STATE_LABELS[j]:>20} <- {SHORT_LABELS[top[0]]} ({top[1]:.4f})"
        if second and second[1] > 0.005:
            line += f", {SHORT_LABELS[second[0]]} ({second[1]:.4f})"
        print(line)

    # Hazard arc: S0 (FL_HAZ) -> ??? -> S0
    print(f"\n  Hazard pulse pattern:")
    print(f"    FL_HAZ self-loop: {P[0][0]:.3f} (10.7% re-hazard)")
    print(f"    FL_HAZ -> AX/EN major: {P[0][4]:.3f} (dominant exit)")
    print(f"    FL_HAZ -> FQ: {P[0][1]:.3f} (secondary exit)")
    print(f"    AX/EN major -> FL_HAZ: {P[4][0]:.3f} (hazard re-entry)")
    print(f"    AX/EN major -> FL_SAFE: {P[4][5]:.3f} (safe flow)")
    print(f"    FL_HAZ/FL_SAFE ratio from AXM: {P[4][0]/P[4][5]:.1f}x")

    # CC routing
    print(f"\n  CC (core control) routing:")
    print(f"    CC self-loop: {P[2][2]:.3f}")
    print(f"    CC -> AX/EN major: {P[2][4]:.3f} (dominant: execution after control)")
    print(f"    CC -> FQ: {P[2][1]:.3f}")
    print(f"    AX/EN major -> CC: {P[4][2]:.3f}")
    print(f"    FQ -> CC: {P[1][2]:.3f}")

    # FQ as scaffold
    print(f"\n  FQ (frequent operators) as scaffold:")
    print(f"    FQ self-loop: {P[1][1]:.3f} (25.1% chain)")
    print(f"    FQ -> AX/EN major: {P[1][4]:.3f} (primary handoff)")
    print(f"    ALL states -> FQ range: [{min(P[i][1] for i in range(n)):.3f}, "
          f"{max(P[i][1] for i in range(n)):.3f}]")

    # Minor vs Major AX/EN
    print(f"\n  Minor vs Major AX/EN split:")
    print(f"    AXm -> AXM: {P[3][4]:.3f}")
    print(f"    AXM -> AXm: {P[4][3]:.3f}")
    print(f"    AXm self-loop: {P[3][3]:.3f}")
    print(f"    AXM self-loop: {P[4][4]:.3f}")
    print(f"    Flow asymmetry: AXm->AXM is {P[3][4]/P[4][3]:.1f}x stronger than reverse")

    # =========================================================
    # Summary
    # =========================================================
    print(f"\n{'='*70}")
    print(f"TOPOLOGY SUMMARY")
    print(f"{'='*70}")
    print(f"\n  GRAPH TYPE: Fully connected (all 36 edges have P > 0)")
    print(f"  but DOMINATED by S4 (AX/EN major, 68% of mass)")
    print(f"")
    print(f"  DOMINANT PATTERN: Hub-and-spoke")
    print(f"    S4 is the universal attractor (all states flow to it >56%)")
    print(f"    S1 (FQ) is the secondary hub (~17-24% from all states)")
    print(f"    S0 (FL_HAZ), S2 (CC), S3 (AXm), S5 (FL_SAFE) are periphery")
    print(f"")
    print(f"  MIXING TIME: {mixing_time:.1f} tokens")
    print(f"    Fast mixing = weak long-range memory")
    print(f"    Consistent with first-order Markov sufficiency (C966)")
    print(f"")
    print(f"  HAZARD ASYMMETRY:")
    print(f"    FL_HAZ entry rate: {P[4][0]:.3f} from AXM")
    print(f"    FL_SAFE entry rate: {P[4][5]:.3f} from AXM")
    print(f"    Hazard is {P[4][0]/P[4][5]:.1f}x more likely than safe flow marking")
    print(f"    Confirms C586 (HAZ/SAFE structural split) at automaton level")

    # Save
    results = {
        'test': 'T6_state_topology',
        'n_states': n,
        'transition_matrix': P.tolist(),
        'state_labels': STATE_LABELS,
        'edge_counts': {
            'strong': len(strong),
            'moderate': len(moderate),
            'weak': len(weak),
            'negligible': len(negligible),
        },
        'self_loops': {SHORT_LABELS[i]: round(float(diag[i]), 4) for i in range(n)},
        'expected_dwell': {SHORT_LABELS[i]: round(float(1/(1-diag[i])), 2) for i in range(n)},
        'stationary_distribution': {SHORT_LABELS[i]: round(float(stationary[i]), 5) for i in range(n)},
        'spectral_gap': round(float(spectral_gap), 6),
        'mixing_time': round(float(mixing_time), 2),
        'eigenvalues': [{'magnitude': round(float(abs(ev)), 6),
                         'real': round(float(ev.real), 6),
                         'imag': round(float(ev.imag), 6)} for ev in evals_sorted],
        'hazard_safe_ratio': round(float(P[4][0] / P[4][5]), 2),
        'topology': 'HUB_AND_SPOKE',
        'hub_state': 'S4 (AX/EN major)',
        'secondary_hub': 'S1 (FQ)',
    }

    with open(RESULTS_DIR / 't6_state_topology.json', 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nSaved to {RESULTS_DIR / 't6_state_topology.json'}")


if __name__ == '__main__':
    run()
