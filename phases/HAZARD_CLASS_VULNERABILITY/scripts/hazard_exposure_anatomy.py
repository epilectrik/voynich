#!/usr/bin/env python3
"""
HAZARD_CLASS_VULNERABILITY - Script 1: Hazard Exposure Anatomy

Analyzes class-level routing to/from the 6 hazard classes (C541: 7,8,9,23,30,31)
and within-role hazard contrasts for EN, FL, FQ.

Sections:
  1. Class-level routing to hazard classes (all 49 classes)
  2. Within-role hazard contrast: EN (8,31 vs 32-49)
  3. Within-role hazard contrast: FL (7,30 vs 38,40) and FQ (9,23 vs 13,14)
  4. Hazard exposure summary (immunity mechanism classification)

Data dependencies:
  - class_token_map.json (CLASS_COSURVIVAL_TEST/results/)
  - scripts/voynich.py (Transcript)

Constraint references:
  C541: 6 hazard classes (7=FL, 30=FL, 8=EN, 31=EN, 9=FQ, 23=FQ)
  C542: Gateway/terminal asymmetry (30=gateway, 31=terminal)
  C554: Hazard classes cluster (dispersion 1.29)
  C586: FL splits FL_HAZ={7,30} vs FL_SAFE={38,40}
  C601: Hazard circuit FL_HAZ->FQ_CONN->EN_CHSH; EN_QO never participates
"""

import json
import sys
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime

PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from scripts.voynich import Transcript, Morphology

# ============================================================
# CONSTANTS
# ============================================================
HAZARD_CLASSES = {7, 8, 9, 23, 30, 31}
HAZARD_FL = {7, 30}
HAZARD_EN = {8, 31}
HAZARD_FQ = {9, 23}

SAFE_FL = {38, 40}
SAFE_FQ = {13, 14}
# Safe EN = all EN minus {8, 31} - computed from data

# ============================================================
# LOAD DATA
# ============================================================
def load_class_token_map():
    path = PROJECT_ROOT / 'phases' / 'CLASS_COSURVIVAL_TEST' / 'results' / 'class_token_map.json'
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def build_b_token_sequences():
    """Build per-folio, per-line token sequences for Currier B."""
    tx = Transcript()
    sequences = defaultdict(list)
    for token in tx.currier_b():
        key = (token.folio, token.line)
        sequences[key].append(token.word)
    return sequences

# ============================================================
# SECTION 1: CLASS-LEVEL ROUTING TO HAZARD CLASSES
# ============================================================
def section1_class_routing(sequences, token_to_class, class_to_role):
    print("=" * 70)
    print("SECTION 1: CLASS-LEVEL ROUTING TO HAZARD CLASSES")
    print("=" * 70)
    print()

    # Build bigram counts: class_i -> class_j
    class_bigrams = defaultdict(lambda: defaultdict(int))
    class_total_successors = defaultdict(int)
    class_total_predecessors = defaultdict(int)
    total_bigrams = 0

    for key in sorted(sequences.keys()):
        words = sequences[key]
        for i in range(len(words) - 1):
            w1, w2 = words[i], words[i + 1]
            c1 = token_to_class.get(w1)
            c2 = token_to_class.get(w2)
            if c1 is not None and c2 is not None:
                class_bigrams[c1][c2] += 1
                class_total_successors[c1] += 1
                class_total_predecessors[c2] += 1
                total_bigrams += 1

    print(f"Total class-level bigrams: {total_bigrams}")
    print()

    # For each class, compute hazard exposure
    all_classes = sorted(set(int(k) for k in class_to_role.keys()))
    results = {}

    print(f"{'Class':>5} {'Role':>6} {'Haz?':>4} {'->Haz':>6} {'->Tot':>6} {'->Haz%':>7} "
          f"{'Haz->':>6} {'Tot->':>6} {'Haz->%':>7} {'Exposure':>9}")
    print("-" * 80)

    for cls in all_classes:
        role = class_to_role.get(str(cls), 'UNK')
        role_short = role[:2] if role else 'UNK'
        is_hazard = cls in HAZARD_CLASSES

        # Transitions TO hazard classes (successors that are hazard)
        to_hazard = sum(class_bigrams[cls].get(h, 0) for h in HAZARD_CLASSES)
        to_total = class_total_successors.get(cls, 0)
        to_hazard_rate = to_hazard / to_total if to_total > 0 else 0.0

        # Transitions FROM hazard classes (predecessors that are hazard)
        from_hazard = sum(class_bigrams[h].get(cls, 0) for h in HAZARD_CLASSES)
        from_total = class_total_predecessors.get(cls, 0)
        from_hazard_rate = from_hazard / from_total if from_total > 0 else 0.0

        # Total exposure = to + from hazard
        exposure = to_hazard + from_hazard

        results[cls] = {
            'role': role,
            'is_hazard': is_hazard,
            'to_hazard_count': to_hazard,
            'to_total': to_total,
            'to_hazard_rate': round(to_hazard_rate, 4),
            'from_hazard_count': from_hazard,
            'from_total': from_total,
            'from_hazard_rate': round(from_hazard_rate, 4),
            'total_exposure': exposure,
        }

        haz_tag = "HAZ" if is_hazard else ""
        print(f"{cls:5d} {role_short:>6} {haz_tag:>4} {to_hazard:6d} {to_total:6d} "
              f"{to_hazard_rate:7.3f} {from_hazard:6d} {from_total:6d} "
              f"{from_hazard_rate:7.3f} {exposure:9d}")

    print()

    # Rank by total exposure
    ranked = sorted(results.items(), key=lambda x: x[1]['total_exposure'], reverse=True)
    print("Top 10 by total hazard exposure:")
    for i, (cls, r) in enumerate(ranked[:10]):
        tag = " [HAZARD]" if r['is_hazard'] else ""
        print(f"  {i+1:2d}. Class {cls:2d} ({r['role'][:2]}): exposure={r['total_exposure']}{tag}")
    print()

    # Chi-squared: does class predict hazard-neighbor rate beyond role?
    # Group safe classes by role, test whether within-role variance in hazard rate is significant
    role_groups = defaultdict(list)
    for cls, r in results.items():
        if cls not in HAZARD_CLASSES:
            role_groups[r['role']].append(r['to_hazard_rate'])

    print("Safe-class hazard routing rate by role:")
    role_stats = {}
    for role in sorted(role_groups.keys()):
        rates = role_groups[role]
        if len(rates) > 0:
            mean_rate = sum(rates) / len(rates)
            min_rate = min(rates)
            max_rate = max(rates)
            std_rate = (sum((r - mean_rate)**2 for r in rates) / len(rates))**0.5 if len(rates) > 1 else 0
            role_stats[role] = {
                'n': len(rates),
                'mean': round(mean_rate, 4),
                'std': round(std_rate, 4),
                'min': round(min_rate, 4),
                'max': round(max_rate, 4),
            }
            print(f"  {role:25s}: n={len(rates):2d}, mean={mean_rate:.4f}, "
                  f"std={std_rate:.4f}, range=[{min_rate:.4f}, {max_rate:.4f}]")
    print()

    # Chi-squared: observed vs expected hazard-neighbor counts
    # For each safe class: observed = to_hazard_count, expected = to_total * global_hazard_rate
    total_to_hazard = sum(r['to_hazard_count'] for r in results.values() if not r['is_hazard'])
    total_to_all = sum(r['to_total'] for r in results.values() if not r['is_hazard'])
    global_hazard_rate = total_to_hazard / total_to_all if total_to_all > 0 else 0

    chi2_class = 0
    chi2_df = 0
    for cls, r in sorted(results.items()):
        if r['is_hazard']:
            continue
        expected = r['to_total'] * global_hazard_rate
        if expected >= 5:
            chi2_class += (r['to_hazard_count'] - expected)**2 / expected
            chi2_df += 1

    chi2_df = max(chi2_df - 1, 1)
    print(f"Chi-squared (class predicts hazard-neighbor rate): chi2={chi2_class:.2f}, df={chi2_df}")
    print(f"Global safe-class hazard routing rate: {global_hazard_rate:.4f}")

    # Also by role
    chi2_role = 0
    for role, stats in role_stats.items():
        role_classes = [cls for cls, r in results.items()
                        if r['role'] == role and cls not in HAZARD_CLASSES]
        expected_role = sum(results[cls]['to_total'] for cls in role_classes) * global_hazard_rate
        observed_role = sum(results[cls]['to_hazard_count'] for cls in role_classes)
        if expected_role >= 5:
            chi2_role += (observed_role - expected_role)**2 / expected_role

    chi2_role_df = max(len(role_stats) - 1, 1)
    print(f"Chi-squared (role predicts hazard-neighbor rate): chi2={chi2_role:.2f}, df={chi2_role_df}")
    print()

    return results, class_bigrams, role_stats


# ============================================================
# SECTION 2: WITHIN-ROLE HAZARD CONTRAST (EN)
# ============================================================
def section2_en_contrast(sequences, token_to_class, class_to_role, class_bigrams,
                         class_token_map):
    print("=" * 70)
    print("SECTION 2: WITHIN-ROLE HAZARD CONTRAST (EN)")
    print("=" * 70)
    print()

    # Identify EN classes
    en_classes = sorted(int(k) for k, v in class_to_role.items() if v == 'ENERGY_OPERATOR')
    en_hazard = sorted(HAZARD_EN)
    en_safe = sorted(set(en_classes) - HAZARD_EN)

    print(f"EN total classes: {len(en_classes)}")
    print(f"EN hazard: {en_hazard}")
    print(f"EN safe: {en_safe}")
    print()

    morph = Morphology()
    class_to_tokens = class_token_map.get('class_to_tokens', {})

    # --- Transition profiles ---
    print("-" * 60)
    print("2A: Transition Profile Comparison (hazard EN vs safe EN)")
    print("-" * 60)

    def transition_profile(cls_set, bigrams):
        """Compute outgoing transition distribution for a set of classes."""
        profile = defaultdict(int)
        total = 0
        for cls in cls_set:
            for target, count in bigrams.get(cls, {}).items():
                profile[target] += count
                total += count
        return profile, total

    haz_profile, haz_total = transition_profile(en_hazard, class_bigrams)
    safe_profile, safe_total = transition_profile(en_safe, class_bigrams)

    # Top targets for hazard vs safe EN
    print(f"\nHazard EN ({en_hazard}) -> top 10 targets (n={haz_total}):")
    for cls, count in sorted(haz_profile.items(), key=lambda x: -x[1])[:10]:
        role = class_to_role.get(str(cls), '?')[:2]
        haz_tag = " [HAZ]" if cls in HAZARD_CLASSES else ""
        print(f"  -> class {cls:2d} ({role}): {count:5d} ({count/haz_total*100:.1f}%){haz_tag}")

    print(f"\nSafe EN ({len(en_safe)} classes) -> top 10 targets (n={safe_total}):")
    for cls, count in sorted(safe_profile.items(), key=lambda x: -x[1])[:10]:
        role = class_to_role.get(str(cls), '?')[:2]
        haz_tag = " [HAZ]" if cls in HAZARD_CLASSES else ""
        print(f"  -> class {cls:2d} ({role}): {count:5d} ({count/safe_total*100:.1f}%){haz_tag}")
    print()

    # --- Token volume ---
    print("-" * 60)
    print("2B: Token Volume (hazard EN vs safe EN)")
    print("-" * 60)

    # Count B tokens per class
    token_counts = Counter()
    for key in sequences:
        for word in sequences[key]:
            cls = token_to_class.get(word)
            if cls is not None:
                token_counts[cls] += 1

    haz_en_tokens = sum(token_counts[c] for c in en_hazard)
    safe_en_tokens = sum(token_counts[c] for c in en_safe)
    print(f"\nHazard EN token count: {haz_en_tokens}")
    print(f"Safe EN token count:   {safe_en_tokens}")
    print(f"Ratio (hazard/safe):   {haz_en_tokens/safe_en_tokens:.3f}" if safe_en_tokens > 0 else "")
    print()

    for cls in en_hazard:
        tokens = class_to_tokens.get(str(cls), [])
        print(f"  Class {cls}: {token_counts[cls]:5d} tokens, members={tokens}")
    print()
    for cls in en_safe:
        tokens = class_to_tokens.get(str(cls), [])
        print(f"  Class {cls}: {token_counts[cls]:5d} tokens, members={tokens[:5]}{'...' if len(tokens)>5 else ''}")
    print()

    # --- Position distributions ---
    print("-" * 60)
    print("2C: Position Distribution (hazard EN vs safe EN)")
    print("-" * 60)

    # Compute per-line position (0-indexed fraction along line)
    haz_positions = []
    safe_positions = []
    haz_initial_count = 0
    haz_final_count = 0
    safe_initial_count = 0
    safe_final_count = 0
    haz_total_pos = 0
    safe_total_pos = 0

    for key in sorted(sequences.keys()):
        words = sequences[key]
        line_len = len(words)
        if line_len == 0:
            continue
        for i, word in enumerate(words):
            cls = token_to_class.get(word)
            if cls is None:
                continue
            pos_frac = i / max(line_len - 1, 1)
            is_initial = (i == 0)
            is_final = (i == line_len - 1)

            if cls in HAZARD_EN:
                haz_positions.append(pos_frac)
                haz_total_pos += 1
                if is_initial:
                    haz_initial_count += 1
                if is_final:
                    haz_final_count += 1
            elif cls in en_safe:
                safe_positions.append(pos_frac)
                safe_total_pos += 1
                if is_initial:
                    safe_initial_count += 1
                if is_final:
                    safe_final_count += 1

    haz_mean_pos = sum(haz_positions) / len(haz_positions) if haz_positions else 0
    safe_mean_pos = sum(safe_positions) / len(safe_positions) if safe_positions else 0

    print(f"\nHazard EN: n={haz_total_pos}, mean_pos={haz_mean_pos:.3f}, "
          f"initial={haz_initial_count} ({haz_initial_count/max(haz_total_pos,1)*100:.1f}%), "
          f"final={haz_final_count} ({haz_final_count/max(haz_total_pos,1)*100:.1f}%)")
    print(f"Safe EN:   n={safe_total_pos}, mean_pos={safe_mean_pos:.3f}, "
          f"initial={safe_initial_count} ({safe_initial_count/max(safe_total_pos,1)*100:.1f}%), "
          f"final={safe_final_count} ({safe_final_count/max(safe_total_pos,1)*100:.1f}%)")

    # Mann-Whitney U test (manual implementation)
    mw_result = mann_whitney_u(haz_positions, safe_positions)
    print(f"Mann-Whitney U: U={mw_result['U']:.0f}, z={mw_result['z']:.3f}, "
          f"p={mw_result['p']:.6f} {'***' if mw_result['p']<0.001 else '**' if mw_result['p']<0.01 else '*' if mw_result['p']<0.05 else 'ns'}")
    print()

    # --- MIDDLE composition ---
    print("-" * 60)
    print("2D: MIDDLE Composition (hazard EN vs safe EN)")
    print("-" * 60)

    haz_middles = set()
    safe_middles = set()
    class_middles = class_token_map.get('class_to_middles', {})

    for cls in en_hazard:
        for m in class_middles.get(str(cls), []):
            if m:
                haz_middles.add(m)
    for cls in en_safe:
        for m in class_middles.get(str(cls), []):
            if m:
                safe_middles.add(m)

    shared = haz_middles & safe_middles
    haz_only = haz_middles - safe_middles
    safe_only = safe_middles - haz_middles

    print(f"\nHazard EN MIDDLEs: {sorted(haz_middles)}")
    print(f"Safe EN MIDDLEs:   {len(safe_middles)} unique")
    print(f"Shared: {sorted(shared)}")
    print(f"Hazard-only: {sorted(haz_only)}")
    print(f"Safe-only count: {len(safe_only)}")

    jaccard = len(shared) / len(haz_middles | safe_middles) if (haz_middles | safe_middles) else 0
    print(f"Jaccard similarity: {jaccard:.3f}")
    print()

    return {
        'en_classes': en_classes,
        'en_hazard': en_hazard,
        'en_safe': en_safe,
        'hazard_en_tokens': haz_en_tokens,
        'safe_en_tokens': safe_en_tokens,
        'hazard_en_mean_position': round(haz_mean_pos, 4),
        'safe_en_mean_position': round(safe_mean_pos, 4),
        'mann_whitney': mw_result,
        'middle_jaccard': round(jaccard, 4),
        'hazard_middles': sorted(haz_middles),
        'safe_middles_count': len(safe_middles),
        'shared_middles': sorted(shared),
        'hazard_only_middles': sorted(haz_only),
        'haz_initial_rate': round(haz_initial_count / max(haz_total_pos, 1), 4),
        'safe_initial_rate': round(safe_initial_count / max(safe_total_pos, 1), 4),
        'haz_final_rate': round(haz_final_count / max(haz_total_pos, 1), 4),
        'safe_final_rate': round(safe_final_count / max(safe_total_pos, 1), 4),
    }


# ============================================================
# SECTION 3: WITHIN-ROLE HAZARD CONTRAST (FL and FQ)
# ============================================================
def section3_fl_fq_contrast(sequences, token_to_class, class_to_role, class_bigrams,
                             class_token_map):
    print("=" * 70)
    print("SECTION 3: WITHIN-ROLE HAZARD CONTRAST (FL and FQ)")
    print("=" * 70)
    print()

    morph = Morphology()
    class_to_tokens = class_token_map.get('class_to_tokens', {})
    class_middles = class_token_map.get('class_to_middles', {})

    # Count B tokens per class
    token_counts = Counter()
    for key in sequences:
        for word in sequences[key]:
            cls = token_to_class.get(word)
            if cls is not None:
                token_counts[cls] += 1

    results = {}

    for role_name, haz_set, safe_set in [
        ('FL', HAZARD_FL, SAFE_FL),
        ('FQ', HAZARD_FQ, SAFE_FQ),
    ]:
        print(f"{'='*40}")
        print(f"  {role_name}: hazard={sorted(haz_set)} vs safe={sorted(safe_set)}")
        print(f"{'='*40}")
        print()

        # --- Transition profiles ---
        print(f"  Transition profiles:")
        for label, cls_set in [('hazard', haz_set), ('safe', safe_set)]:
            profile = defaultdict(int)
            total = 0
            for cls in cls_set:
                for target, count in class_bigrams.get(cls, {}).items():
                    profile[target] += count
                    total += count
            print(f"    {label} {role_name} -> top 5 targets (n={total}):")
            for cls, count in sorted(profile.items(), key=lambda x: -x[1])[:5]:
                r = class_to_role.get(str(cls), '?')[:2]
                haz_tag = " [HAZ]" if cls in HAZARD_CLASSES else ""
                pct = count / total * 100 if total > 0 else 0
                print(f"      -> class {cls:2d} ({r}): {count:5d} ({pct:.1f}%){haz_tag}")
        print()

        # --- Token volume ---
        haz_vol = sum(token_counts[c] for c in haz_set)
        safe_vol = sum(token_counts[c] for c in safe_set)
        print(f"  Token volume: hazard={haz_vol}, safe={safe_vol}")

        for cls in sorted(haz_set | safe_set):
            tokens = class_to_tokens.get(str(cls), [])
            tag = "HAZ" if cls in haz_set else "safe"
            print(f"    Class {cls:2d} [{tag}]: {token_counts[cls]:5d} tokens, members={tokens}")
        print()

        # --- Position ---
        haz_positions = []
        safe_positions = []

        for key in sorted(sequences.keys()):
            words = sequences[key]
            line_len = len(words)
            if line_len == 0:
                continue
            for i, word in enumerate(words):
                cls = token_to_class.get(word)
                if cls is None:
                    continue
                pos_frac = i / max(line_len - 1, 1)
                if cls in haz_set:
                    haz_positions.append(pos_frac)
                elif cls in safe_set:
                    safe_positions.append(pos_frac)

        haz_mean = sum(haz_positions) / len(haz_positions) if haz_positions else 0
        safe_mean = sum(safe_positions) / len(safe_positions) if safe_positions else 0
        print(f"  Position: hazard mean={haz_mean:.3f} (n={len(haz_positions)}), "
              f"safe mean={safe_mean:.3f} (n={len(safe_positions)})")

        if len(haz_positions) >= 5 and len(safe_positions) >= 5:
            mw = mann_whitney_u(haz_positions, safe_positions)
            print(f"  Mann-Whitney: U={mw['U']:.0f}, z={mw['z']:.3f}, p={mw['p']:.6f}")
        else:
            mw = {'U': 0, 'z': 0, 'p': 1.0, 'note': 'insufficient_samples'}
            print(f"  Mann-Whitney: insufficient samples")
        print()

        # --- MIDDLE composition ---
        haz_middles = set()
        safe_middles = set()
        for cls in haz_set:
            for m in class_middles.get(str(cls), []):
                if m:
                    haz_middles.add(m)
        for cls in safe_set:
            for m in class_middles.get(str(cls), []):
                if m:
                    safe_middles.add(m)

        shared = haz_middles & safe_middles
        jaccard = len(shared) / len(haz_middles | safe_middles) if (haz_middles | safe_middles) else 0
        print(f"  MIDDLE: hazard={sorted(haz_middles)}, safe={sorted(safe_middles)}")
        print(f"  Shared: {sorted(shared)}, Jaccard={jaccard:.3f}")
        print()

        results[role_name] = {
            'hazard_classes': sorted(haz_set),
            'safe_classes': sorted(safe_set),
            'hazard_volume': haz_vol,
            'safe_volume': safe_vol,
            'hazard_mean_position': round(haz_mean, 4),
            'safe_mean_position': round(safe_mean, 4),
            'mann_whitney': mw,
            'hazard_middles': sorted(haz_middles),
            'safe_middles': sorted(safe_middles),
            'middle_jaccard': round(jaccard, 4),
        }

    return results


# ============================================================
# SECTION 4: HAZARD EXPOSURE SUMMARY
# ============================================================
def section4_summary(exposure_results, class_to_role):
    print("=" * 70)
    print("SECTION 4: HAZARD EXPOSURE SUMMARY")
    print("=" * 70)
    print()

    all_classes = sorted(int(k) for k in class_to_role.keys())
    safe_classes = [c for c in all_classes if c not in HAZARD_CLASSES]

    # Classify immunity mechanism
    # Role exclusion: CC/AX roles (they contain 0 hazard classes within their role)
    role_excluded = []
    subgroup_excluded = []
    other_safe = []

    # CC role classes
    cc_classes = sorted(int(k) for k, v in class_to_role.items() if v == 'CORE_CONTROL')
    ax_classes = sorted(int(k) for k, v in class_to_role.items() if v == 'AUXILIARY')

    # EN safe subgroups (EN_QO = classes containing qo-prefix tokens: 32,33,36,44,45,46,49)
    # FL_SAFE = {38, 40}
    # FQ_PAIR = {13, 14}
    en_classes = sorted(int(k) for k, v in class_to_role.items() if v == 'ENERGY_OPERATOR')
    en_safe = sorted(set(en_classes) - HAZARD_EN)

    fl_classes = sorted(int(k) for k, v in class_to_role.items() if v == 'FLOW_OPERATOR')
    fq_classes = sorted(int(k) for k, v in class_to_role.items() if v == 'FREQUENT_OPERATOR')

    immunity = {}
    for cls in safe_classes:
        role = class_to_role.get(str(cls), 'UNK')
        if role in ('CORE_CONTROL', 'AUXILIARY'):
            mechanism = 'role_exclusion'
            reason = f'{role} has 0 hazard classes'
        elif role == 'FLOW_OPERATOR' and cls in SAFE_FL:
            mechanism = 'subgroup_exclusion'
            reason = 'FL_SAFE subgroup (C586)'
        elif role == 'FREQUENT_OPERATOR' and cls in SAFE_FQ:
            mechanism = 'subgroup_exclusion'
            reason = 'FQ_PAIR subgroup (C601)'
        elif role == 'ENERGY_OPERATOR' and cls in en_safe:
            mechanism = 'subgroup_exclusion'
            reason = 'EN safe subgroup (not EN_CHSH)'
        else:
            mechanism = 'other'
            reason = 'unknown'

        immunity[cls] = {
            'class': cls,
            'role': role,
            'mechanism': mechanism,
            'reason': reason,
        }

    # Count by mechanism
    mechanism_counts = Counter(v['mechanism'] for v in immunity.values())

    print("Immunity mechanism classification:")
    print(f"  Role exclusion (CC/AX):  {mechanism_counts.get('role_exclusion', 0)} classes")
    print(f"  Sub-group exclusion:     {mechanism_counts.get('subgroup_exclusion', 0)} classes")
    print(f"  Other:                   {mechanism_counts.get('other', 0)} classes")
    print(f"  Total safe:              {len(safe_classes)} classes")
    print()

    # By role breakdown
    print("By role:")
    for role in sorted(set(class_to_role.values())):
        role_cls = sorted(int(k) for k, v in class_to_role.items() if v == role)
        role_safe = [c for c in role_cls if c not in HAZARD_CLASSES]
        role_haz = [c for c in role_cls if c in HAZARD_CLASSES]
        mechanisms = Counter(immunity[c]['mechanism'] for c in role_safe)
        print(f"  {role:25s}: {len(role_cls)} total, {len(role_haz)} hazard, "
              f"{len(role_safe)} safe -> {dict(mechanisms)}")
    print()

    # Detail for each safe class
    print("Safe class detail:")
    for cls in safe_classes:
        info = immunity[cls]
        exp = exposure_results.get(cls, {})
        to_rate = exp.get('to_hazard_rate', 0)
        from_rate = exp.get('from_hazard_rate', 0)
        print(f"  Class {cls:2d} ({info['role'][:2]}): {info['mechanism']:20s} "
              f"to_haz={to_rate:.3f} from_haz={from_rate:.3f} [{info['reason']}]")
    print()

    return {
        'mechanism_counts': dict(mechanism_counts),
        'immunity_details': {str(k): v for k, v in immunity.items()},
        'role_breakdown': {
            role: {
                'total': len([k for k, v in class_to_role.items() if v == role]),
                'hazard': len([int(k) for k, v in class_to_role.items()
                              if v == role and int(k) in HAZARD_CLASSES]),
                'safe': len([int(k) for k, v in class_to_role.items()
                            if v == role and int(k) not in HAZARD_CLASSES]),
            }
            for role in sorted(set(class_to_role.values()))
        },
    }


# ============================================================
# UTILITY: MANN-WHITNEY U TEST
# ============================================================
def mann_whitney_u(x, y):
    """Manual Mann-Whitney U test with normal approximation."""
    import math
    nx, ny = len(x), len(y)
    if nx == 0 or ny == 0:
        return {'U': 0, 'z': 0, 'p': 1.0}

    # Combine and rank
    combined = [(v, 'x') for v in x] + [(v, 'y') for v in y]
    combined.sort(key=lambda t: t[0])

    # Assign ranks with tie handling
    n = len(combined)
    ranks = [0.0] * n
    i = 0
    while i < n:
        j = i
        while j < n and combined[j][0] == combined[i][0]:
            j += 1
        avg_rank = (i + j + 1) / 2  # 1-indexed average
        for k in range(i, j):
            ranks[k] = avg_rank
        i = j

    # Sum ranks for x
    r1 = sum(ranks[k] for k in range(n) if combined[k][1] == 'x')

    u1 = r1 - nx * (nx + 1) / 2
    u2 = nx * ny - u1
    U = min(u1, u2)

    # Normal approximation
    mu = nx * ny / 2
    sigma = math.sqrt(nx * ny * (nx + ny + 1) / 12)
    if sigma == 0:
        return {'U': U, 'z': 0, 'p': 1.0}

    z = (U - mu) / sigma
    # Two-tailed p-value approximation
    p = 2 * (1 - normal_cdf(abs(z)))

    return {'U': round(U, 2), 'z': round(z, 4), 'p': round(p, 6)}


def normal_cdf(x):
    """Approximation of the standard normal CDF."""
    import math
    return 0.5 * (1 + math.erf(x / math.sqrt(2)))


# ============================================================
# MAIN
# ============================================================
def main():
    print("=" * 70)
    print("HAZARD CLASS VULNERABILITY: EXPOSURE ANATOMY")
    print("=" * 70)
    print()

    # Load data
    print("Loading data...")
    ctm = load_class_token_map()
    token_to_class = {k: int(v) for k, v in ctm['token_to_class'].items()}
    class_to_role = ctm['class_to_role']
    print(f"  Token-to-class: {len(token_to_class)} entries")
    print(f"  Classes: {len(class_to_role)}")
    print(f"  Hazard classes: {sorted(HAZARD_CLASSES)}")
    print()

    print("Loading Currier B sequences...")
    sequences = build_b_token_sequences()
    total_tokens = sum(len(v) for v in sequences.values())
    print(f"  Lines: {len(sequences)}")
    print(f"  Total tokens: {total_tokens}")
    print()

    # Section 1
    exposure_results, class_bigrams, role_stats = section1_class_routing(
        sequences, token_to_class, class_to_role)

    # Section 2
    en_results = section2_en_contrast(
        sequences, token_to_class, class_to_role, class_bigrams, ctm)

    # Section 3
    fl_fq_results = section3_fl_fq_contrast(
        sequences, token_to_class, class_to_role, class_bigrams, ctm)

    # Section 4
    summary_results = section4_summary(exposure_results, class_to_role)

    # ============================================================
    # SAVE RESULTS
    # ============================================================
    output = {
        'metadata': {
            'phase': 'HAZARD_CLASS_VULNERABILITY',
            'script': 'hazard_exposure_anatomy',
            'timestamp': datetime.now().isoformat(),
            'hazard_classes': sorted(HAZARD_CLASSES),
            'total_classes': len(class_to_role),
            'total_b_tokens': total_tokens,
            'total_lines': len(sequences),
        },
        'section1_class_routing': {
            'per_class': {str(k): v for k, v in exposure_results.items()},
            'role_stats': role_stats,
        },
        'section2_en_contrast': en_results,
        'section3_fl_fq_contrast': fl_fq_results,
        'section4_summary': summary_results,
    }

    output_path = PROJECT_ROOT / 'phases' / 'HAZARD_CLASS_VULNERABILITY' / 'results' / 'hazard_exposure_anatomy.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2)

    print(f"\nResults saved to {output_path}")


if __name__ == '__main__':
    main()
