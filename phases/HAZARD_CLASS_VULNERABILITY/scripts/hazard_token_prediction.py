#!/usr/bin/env python3
"""
HAZARD_CLASS_VULNERABILITY - Script 2: Hazard Token Prediction

Analyzes morphological predictors of hazard participation and whether
forbidden-transition tokens can be distinguished from safe tokens.

Sections:
  1. Morphological profile of hazard tokens (the ~15 tokens in forbidden transitions)
  2. MIDDLE overlap between hazard and safe classes within EN, FL, FQ
  3. Position distribution of hazard tokens
  4. Hazard prediction model (can morphology predict hazard participation?)

Data dependencies:
  - class_token_map.json (CLASS_COSURVIVAL_TEST/results/)
  - phase18a_forbidden_inventory.json (phases/15-20_kernel_grammar/)
  - scripts/voynich.py (Transcript, Morphology)

Constraint references:
  C541: 6 hazard classes
  C109: 17 forbidden transitions
  C601: Hazard circuit
"""

import json
import sys
import math
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime

PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from scripts.voynich import Transcript, Morphology

# ============================================================
# CONSTANTS
# ============================================================
HAZARD_CLASSES = {7, 8, 9, 23, 30, 31}
HAZARD_FL = {7, 30}
HAZARD_EN = {8, 31}
HAZARD_FQ = {9, 23}
SAFE_FL = {38, 40}
SAFE_FQ = {13, 14}

# ============================================================
# LOAD DATA
# ============================================================
def load_class_token_map():
    path = PROJECT_ROOT / 'phases' / 'CLASS_COSURVIVAL_TEST' / 'results' / 'class_token_map.json'
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_forbidden_inventory():
    path = PROJECT_ROOT / 'phases' / '15-20_kernel_grammar' / 'phase18a_forbidden_inventory.json'
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def build_b_token_sequences():
    """Build per-folio, per-line token sequences for Currier B."""
    tx = Transcript()
    sequences = defaultdict(list)
    for token in tx.currier_b():
        key = (token.folio, token.line)
        sequences[key].append(token.word)
    return sequences

# ============================================================
# SECTION 1: MORPHOLOGICAL PROFILE OF HAZARD TOKENS
# ============================================================
def section1_morph_profile(forbidden_inv, token_to_class, class_to_tokens, morph):
    print("=" * 70)
    print("SECTION 1: MORPHOLOGICAL PROFILE OF HAZARD TOKENS")
    print("=" * 70)
    print()

    # Extract unique tokens involved in forbidden transitions
    transitions = forbidden_inv['transitions']
    forbidden_sources = set()
    forbidden_targets = set()
    for t in transitions:
        forbidden_sources.add(t['source'])
        forbidden_targets.add(t['target'])

    forbidden_tokens = forbidden_sources | forbidden_targets
    print(f"Forbidden transition tokens: {len(forbidden_tokens)}")
    print(f"  Sources: {sorted(forbidden_sources)}")
    print(f"  Targets: {sorted(forbidden_targets)}")
    print()

    # Morphological analysis of each forbidden token
    print("-" * 60)
    print("Morphological decomposition of forbidden tokens:")
    print("-" * 60)
    print(f"{'Token':>12} {'Class':>5} {'Role':>6} {'Art':>4} {'Pre':>4} {'Mid':>6} {'Suf':>6} {'Len':>3}")
    print("-" * 60)

    forbidden_profiles = {}
    for token in sorted(forbidden_tokens):
        cls = token_to_class.get(token)
        m = morph.extract(token)
        forbidden_profiles[token] = {
            'class': cls,
            'articulator': m.articulator,
            'prefix': m.prefix,
            'middle': m.middle,
            'suffix': m.suffix,
            'length': len(token),
            'has_articulator': m.has_articulator,
            'has_prefix': m.has_prefix,
            'has_suffix': m.suffix is not None,
        }
        art = m.articulator or '-'
        pre = m.prefix or '-'
        mid = m.middle or '-'
        suf = m.suffix or '-'
        print(f"{token:>12} {cls or '?':>5} {art:>4} {pre:>4} {mid:>6} {suf:>6} {len(token):3d}")
    print()

    # Compare to all tokens in the SAME hazard classes
    print("-" * 60)
    print("Comparison: forbidden tokens vs all tokens in same class")
    print("-" * 60)

    for cls in sorted(HAZARD_CLASSES):
        members = class_to_tokens.get(str(cls), [])
        forbidden_in_class = [t for t in members if t in forbidden_tokens]
        safe_in_class = [t for t in members if t not in forbidden_tokens]

        if not forbidden_in_class:
            continue

        # Morphological features
        forb_lengths = [len(t) for t in forbidden_in_class]
        safe_lengths = [len(t) for t in safe_in_class] if safe_in_class else []

        forb_suffix_rate = sum(1 for t in forbidden_in_class
                               if morph.extract(t).suffix is not None) / len(forbidden_in_class)
        safe_suffix_rate = (sum(1 for t in safe_in_class
                                if morph.extract(t).suffix is not None) / len(safe_in_class)
                           if safe_in_class else 0)

        forb_art_rate = sum(1 for t in forbidden_in_class
                            if morph.extract(t).has_articulator) / len(forbidden_in_class)
        safe_art_rate = (sum(1 for t in safe_in_class
                             if morph.extract(t).has_articulator) / len(safe_in_class)
                        if safe_in_class else 0)

        forb_middles = set(morph.extract(t).middle for t in forbidden_in_class if morph.extract(t).middle)
        safe_middles = set(morph.extract(t).middle for t in safe_in_class if morph.extract(t).middle)

        print(f"\nClass {cls} ({len(members)} members: {len(forbidden_in_class)} forbidden, {len(safe_in_class)} safe)")
        print(f"  Forbidden: {forbidden_in_class}")
        print(f"  Safe:      {safe_in_class}")
        print(f"  Length:    forbidden={sum(forb_lengths)/len(forb_lengths):.1f}, "
              f"safe={sum(safe_lengths)/len(safe_lengths):.1f}" if safe_lengths else
              f"  Length:    forbidden={sum(forb_lengths)/len(forb_lengths):.1f}, safe=n/a")
        print(f"  Suffix%:   forbidden={forb_suffix_rate:.2f}, safe={safe_suffix_rate:.2f}")
        print(f"  Articulat: forbidden={forb_art_rate:.2f}, safe={safe_art_rate:.2f}")
        print(f"  MIDDLEs:   forbidden={sorted(forb_middles)}, safe={sorted(safe_middles)}")

    print()

    # Summary features across all forbidden tokens
    all_lengths = [len(t) for t in forbidden_tokens]
    all_suffix_rate = sum(1 for t in forbidden_tokens
                          if morph.extract(t).suffix is not None) / len(forbidden_tokens)
    all_art_rate = sum(1 for t in forbidden_tokens
                       if morph.extract(t).has_articulator) / len(forbidden_tokens)
    all_prefix_rate = sum(1 for t in forbidden_tokens
                          if morph.extract(t).has_prefix) / len(forbidden_tokens)

    print("Overall forbidden token profile:")
    print(f"  Mean length: {sum(all_lengths)/len(all_lengths):.1f}")
    print(f"  Suffix rate: {all_suffix_rate:.2f}")
    print(f"  Articulator rate: {all_art_rate:.2f}")
    print(f"  Prefix rate: {all_prefix_rate:.2f}")
    print()

    return {
        'forbidden_tokens': sorted(forbidden_tokens),
        'forbidden_sources': sorted(forbidden_sources),
        'forbidden_targets': sorted(forbidden_targets),
        'profiles': forbidden_profiles,
        'summary': {
            'mean_length': round(sum(all_lengths) / len(all_lengths), 2),
            'suffix_rate': round(all_suffix_rate, 4),
            'articulator_rate': round(all_art_rate, 4),
            'prefix_rate': round(all_prefix_rate, 4),
        },
    }


# ============================================================
# SECTION 2: MIDDLE OVERLAP BETWEEN HAZARD AND SAFE CLASSES
# ============================================================
def section2_middle_overlap(class_to_tokens, class_to_role, class_middles, morph):
    print("=" * 70)
    print("SECTION 2: MIDDLE OVERLAP BETWEEN HAZARD AND SAFE CLASSES")
    print("=" * 70)
    print()

    results = {}

    for role_name, haz_set, safe_set in [
        ('EN', HAZARD_EN, None),
        ('FL', HAZARD_FL, SAFE_FL),
        ('FQ', HAZARD_FQ, SAFE_FQ),
    ]:
        # For EN, compute safe set from data
        if safe_set is None:
            en_all = set(int(k) for k, v in class_to_role.items() if v == 'ENERGY_OPERATOR')
            safe_set = en_all - HAZARD_EN

        print(f"--- {role_name}: hazard={sorted(haz_set)} vs safe={sorted(safe_set)} ---")
        print()

        # Collect MIDDLEs
        haz_middles = set()
        safe_middles = set()
        haz_middle_sources = defaultdict(set)  # middle -> which hazard classes
        safe_middle_sources = defaultdict(set)  # middle -> which safe classes

        for cls in haz_set:
            for m in class_middles.get(str(cls), []):
                if m:
                    haz_middles.add(m)
                    haz_middle_sources[m].add(cls)

        for cls in safe_set:
            for m in class_middles.get(str(cls), []):
                if m:
                    safe_middles.add(m)
                    safe_middle_sources[m].add(cls)

        shared = haz_middles & safe_middles
        haz_only = haz_middles - safe_middles
        safe_only = safe_middles - haz_middles
        jaccard = len(shared) / len(haz_middles | safe_middles) if (haz_middles | safe_middles) else 0

        print(f"  Hazard MIDDLEs ({len(haz_middles)}): {sorted(haz_middles)}")
        print(f"  Safe MIDDLEs ({len(safe_middles)}): {sorted(safe_middles)}")
        print(f"  Shared ({len(shared)}): {sorted(shared)}")
        print(f"  Hazard-exclusive ({len(haz_only)}): {sorted(haz_only)}")
        print(f"  Safe-exclusive ({len(safe_only)}): {len(safe_only)} MIDDLEs")
        print(f"  Jaccard similarity: {jaccard:.3f}")
        print()

        # Properties of hazard-exclusive MIDDLEs
        if haz_only:
            print(f"  Hazard-exclusive MIDDLE properties:")
            for m in sorted(haz_only):
                sources = haz_middle_sources[m]
                print(f"    '{m}' (len={len(m)}): from classes {sorted(sources)}")
            print()

        results[role_name] = {
            'hazard_middles': sorted(haz_middles),
            'safe_middles': sorted(safe_middles),
            'shared': sorted(shared),
            'hazard_exclusive': sorted(haz_only),
            'safe_exclusive_count': len(safe_only),
            'jaccard': round(jaccard, 4),
        }

    return results


# ============================================================
# SECTION 3: POSITION DISTRIBUTION
# ============================================================
def section3_position(sequences, token_to_class, forbidden_tokens, class_to_role):
    print("=" * 70)
    print("SECTION 3: POSITION DISTRIBUTION OF HAZARD TOKENS")
    print("=" * 70)
    print()

    # Collect positions for forbidden tokens vs safe tokens in same roles
    forbidden_positions = []
    safe_hazard_class_positions = []  # tokens in hazard classes but not forbidden
    safe_same_role_positions = defaultdict(list)  # by role

    # Build sets for quick lookup
    forbidden_set = set(forbidden_tokens)

    # Determine which roles the forbidden tokens belong to
    forbidden_roles = set()
    for t in forbidden_tokens:
        cls = token_to_class.get(t)
        if cls is not None:
            role = class_to_role.get(str(cls))
            if role:
                forbidden_roles.add(role)

    # Pre-compute class sets by role (safe classes only)
    safe_classes_by_role = defaultdict(set)
    for k, v in class_to_role.items():
        cls = int(k)
        if cls not in HAZARD_CLASSES:
            safe_classes_by_role[v].add(cls)

    for key in sorted(sequences.keys()):
        words = sequences[key]
        line_len = len(words)
        if line_len == 0:
            continue
        for i, word in enumerate(words):
            cls = token_to_class.get(word)
            if cls is None:
                continue
            pos_frac = i / max(line_len - 1, 1)
            is_initial = (i == 0)
            is_final = (i == line_len - 1)

            if word in forbidden_set:
                forbidden_positions.append({
                    'pos': pos_frac,
                    'initial': is_initial,
                    'final': is_final,
                    'word': word,
                })
            elif cls in HAZARD_CLASSES:
                safe_hazard_class_positions.append({
                    'pos': pos_frac,
                    'initial': is_initial,
                    'final': is_final,
                })
            else:
                role = class_to_role.get(str(cls))
                if role in forbidden_roles:
                    safe_same_role_positions[role].append({
                        'pos': pos_frac,
                        'initial': is_initial,
                        'final': is_final,
                    })

    # --- Forbidden token positions ---
    forb_pos = [p['pos'] for p in forbidden_positions]
    forb_init = sum(1 for p in forbidden_positions if p['initial'])
    forb_final = sum(1 for p in forbidden_positions if p['final'])
    n_forb = len(forb_pos)

    print(f"Forbidden tokens: n={n_forb}")
    if n_forb > 0:
        print(f"  Mean position: {sum(forb_pos)/n_forb:.3f}")
        print(f"  Line-initial: {forb_init} ({forb_init/n_forb*100:.1f}%)")
        print(f"  Line-final: {forb_final} ({forb_final/n_forb*100:.1f}%)")

        # Per-token position breakdown
        token_positions = defaultdict(list)
        for p in forbidden_positions:
            token_positions[p['word']].append(p['pos'])

        print(f"\n  Per-token breakdown:")
        for word in sorted(token_positions.keys()):
            positions = token_positions[word]
            mean_p = sum(positions) / len(positions)
            print(f"    {word:>10}: n={len(positions):5d}, mean_pos={mean_p:.3f}")
    print()

    # --- Tokens in hazard classes but not forbidden ---
    shc_pos = [p['pos'] for p in safe_hazard_class_positions]
    shc_init = sum(1 for p in safe_hazard_class_positions if p['initial'])
    n_shc = len(shc_pos)

    print(f"Hazard-class tokens (not forbidden): n={n_shc}")
    if n_shc > 0:
        print(f"  Mean position: {sum(shc_pos)/n_shc:.3f}")
        print(f"  Line-initial: {shc_init} ({shc_init/n_shc*100:.1f}%)")
    print()

    # --- Mann-Whitney: forbidden vs hazard-class non-forbidden ---
    if n_forb >= 5 and n_shc >= 5:
        mw = mann_whitney_u(forb_pos, shc_pos)
        print(f"Mann-Whitney (forbidden vs non-forbidden in hazard classes):")
        print(f"  U={mw['U']:.0f}, z={mw['z']:.3f}, p={mw['p']:.6f}")
    else:
        mw = {'U': 0, 'z': 0, 'p': 1.0, 'note': 'insufficient_samples'}
        print(f"Mann-Whitney: insufficient samples")
    print()

    # --- Compare by role ---
    print("Comparison to safe tokens in same roles:")
    role_mw = {}
    for role in sorted(safe_same_role_positions.keys()):
        safe_pos = [p['pos'] for p in safe_same_role_positions[role]]
        if len(safe_pos) >= 5 and n_forb >= 5:
            mw_r = mann_whitney_u(forb_pos, safe_pos)
            role_mw[role] = mw_r
            print(f"  {role}: n_safe={len(safe_pos)}, safe_mean={sum(safe_pos)/len(safe_pos):.3f}, "
                  f"U={mw_r['U']:.0f}, z={mw_r['z']:.3f}, p={mw_r['p']:.6f}")
        else:
            print(f"  {role}: n_safe={len(safe_pos)} (insufficient)")
    print()

    return {
        'forbidden_positions': {
            'n': n_forb,
            'mean': round(sum(forb_pos) / n_forb, 4) if n_forb > 0 else None,
            'initial_count': forb_init,
            'final_count': forb_final,
            'initial_rate': round(forb_init / max(n_forb, 1), 4),
            'final_rate': round(forb_final / max(n_forb, 1), 4),
        },
        'hazard_class_non_forbidden': {
            'n': n_shc,
            'mean': round(sum(shc_pos) / n_shc, 4) if n_shc > 0 else None,
        },
        'mann_whitney_forbidden_vs_hazclass': mw,
        'role_comparisons': role_mw,
    }


# ============================================================
# SECTION 4: HAZARD PREDICTION MODEL
# ============================================================
def section4_prediction(forbidden_tokens_set, token_to_class, class_to_tokens,
                        class_to_role, morph):
    print("=" * 70)
    print("SECTION 4: HAZARD PREDICTION MODEL")
    print("=" * 70)
    print()

    # For each token in EN/FL/FQ hazard classes, compute features
    # Binary classification: forbidden-participant vs non-participant

    features = []
    for cls in sorted(HAZARD_CLASSES):
        members = class_to_tokens.get(str(cls), [])
        role = class_to_role.get(str(cls), 'UNK')
        for token in members:
            m = morph.extract(token)
            is_forbidden = token in forbidden_tokens_set
            mid_len = len(m.middle) if m.middle else 0
            features.append({
                'token': token,
                'class': cls,
                'role': role,
                'is_forbidden': is_forbidden,
                'length': len(token),
                'middle_length': mid_len,
                'has_suffix': m.suffix is not None,
                'has_prefix': m.has_prefix,
                'has_articulator': m.has_articulator,
                'middle': m.middle,
                'prefix': m.prefix,
                'suffix': m.suffix,
            })

    n_forbidden = sum(1 for f in features if f['is_forbidden'])
    n_safe = sum(1 for f in features if not f['is_forbidden'])
    print(f"Hazard-class tokens: {len(features)} ({n_forbidden} forbidden, {n_safe} non-forbidden)")
    print()

    # --- Feature comparison ---
    print("-" * 60)
    print("Feature comparison: forbidden vs non-forbidden (within hazard classes)")
    print("-" * 60)

    forb = [f for f in features if f['is_forbidden']]
    safe = [f for f in features if not f['is_forbidden']]

    comparisons = {}
    for feature_name in ['length', 'middle_length', 'has_suffix', 'has_prefix', 'has_articulator']:
        forb_vals = [f[feature_name] for f in forb]
        safe_vals = [f[feature_name] for f in safe]

        if isinstance(forb_vals[0], bool):
            forb_rate = sum(forb_vals) / len(forb_vals)
            safe_rate = sum(safe_vals) / len(safe_vals) if safe_vals else 0
            print(f"  {feature_name:20s}: forbidden={forb_rate:.2f}, safe={safe_rate:.2f}")
            comparisons[feature_name] = {
                'forbidden_rate': round(forb_rate, 4),
                'safe_rate': round(safe_rate, 4),
            }
        else:
            forb_mean = sum(forb_vals) / len(forb_vals)
            safe_mean = sum(safe_vals) / len(safe_vals) if safe_vals else 0
            print(f"  {feature_name:20s}: forbidden_mean={forb_mean:.2f}, safe_mean={safe_mean:.2f}")
            comparisons[feature_name] = {
                'forbidden_mean': round(forb_mean, 2),
                'safe_mean': round(safe_mean, 2),
            }
    print()

    # --- Simple discriminant analysis ---
    print("-" * 60)
    print("Simple discriminant: can any single feature separate hazard from safe?")
    print("-" * 60)

    # For each feature, find the best threshold
    best_feature = None
    best_accuracy = 0.5
    best_threshold = None

    for feature_name in ['length', 'middle_length']:
        forb_vals = sorted(set(f[feature_name] for f in forb))
        all_vals = sorted(set(f[feature_name] for f in features))

        for threshold in all_vals:
            # Predict: forbidden if value <= threshold
            correct = 0
            for f in features:
                predicted_forb = f[feature_name] <= threshold
                if predicted_forb == f['is_forbidden']:
                    correct += 1
            acc = correct / len(features)
            if acc > best_accuracy:
                best_accuracy = acc
                best_feature = f"{feature_name} <= {threshold}"
                best_threshold = threshold

            # Also try: forbidden if value >= threshold
            correct2 = 0
            for f in features:
                predicted_forb = f[feature_name] >= threshold
                if predicted_forb == f['is_forbidden']:
                    correct2 += 1
            acc2 = correct2 / len(features)
            if acc2 > best_accuracy:
                best_accuracy = acc2
                best_feature = f"{feature_name} >= {threshold}"
                best_threshold = threshold

    for feature_name in ['has_suffix', 'has_prefix', 'has_articulator']:
        # Predict: forbidden if has_feature == True
        correct_t = sum(1 for f in features if f[feature_name] == f['is_forbidden'])
        correct_f = sum(1 for f in features if (not f[feature_name]) == f['is_forbidden'])
        acc_t = correct_t / len(features)
        acc_f = correct_f / len(features)
        for acc, rule in [(acc_t, f"{feature_name} == True"),
                          (acc_f, f"{feature_name} == False")]:
            if acc > best_accuracy:
                best_accuracy = acc
                best_feature = rule

    print(f"Best single-feature discriminant:")
    print(f"  Rule: {best_feature}")
    print(f"  Accuracy: {best_accuracy:.3f}")
    print(f"  Baseline (majority class): {max(n_forbidden, n_safe) / len(features):.3f}")
    print()

    is_predictable = best_accuracy > max(n_forbidden, n_safe) / len(features) + 0.1
    if is_predictable:
        print("FINDING: Morphological features provide modest discrimination")
    else:
        print("FINDING: No single morphological feature separates forbidden from safe tokens")
        print("         Hazard participation is LEXICALLY SPECIFIC, not morphologically predictable")
    print()

    # --- MIDDLE identity analysis ---
    print("-" * 60)
    print("MIDDLE identity: which MIDDLEs appear in forbidden tokens?")
    print("-" * 60)

    forb_middles = Counter(f['middle'] for f in forb if f['middle'])
    safe_middles = Counter(f['middle'] for f in safe if f['middle'])

    print(f"\nForbidden token MIDDLEs: {dict(forb_middles)}")
    print(f"Non-forbidden token MIDDLEs: {dict(safe_middles)}")

    forb_mid_set = set(forb_middles.keys())
    safe_mid_set = set(safe_middles.keys())
    shared = forb_mid_set & safe_mid_set
    forb_only = forb_mid_set - safe_mid_set
    safe_only = safe_mid_set - forb_mid_set

    print(f"\nShared MIDDLEs: {sorted(shared)}")
    print(f"Forbidden-only MIDDLEs: {sorted(forb_only)}")
    print(f"Safe-only MIDDLEs: {sorted(safe_only)}")
    print()

    return {
        'n_forbidden': n_forbidden,
        'n_safe': n_safe,
        'feature_comparisons': comparisons,
        'best_discriminant': {
            'rule': best_feature,
            'accuracy': round(best_accuracy, 4),
            'baseline': round(max(n_forbidden, n_safe) / len(features), 4),
            'is_predictable': is_predictable,
        },
        'middle_analysis': {
            'forbidden_middles': dict(forb_middles),
            'safe_middles': dict(safe_middles),
            'shared': sorted(shared),
            'forbidden_only': sorted(forb_only),
            'safe_only': sorted(safe_only),
        },
        'features': features,
    }


# ============================================================
# UTILITY: MANN-WHITNEY U TEST
# ============================================================
def mann_whitney_u(x, y):
    """Manual Mann-Whitney U test with normal approximation."""
    nx, ny = len(x), len(y)
    if nx == 0 or ny == 0:
        return {'U': 0, 'z': 0, 'p': 1.0}

    combined = [(v, 'x') for v in x] + [(v, 'y') for v in y]
    combined.sort(key=lambda t: t[0])

    n = len(combined)
    ranks = [0.0] * n
    i = 0
    while i < n:
        j = i
        while j < n and combined[j][0] == combined[i][0]:
            j += 1
        avg_rank = (i + j + 1) / 2
        for k in range(i, j):
            ranks[k] = avg_rank
        i = j

    r1 = sum(ranks[k] for k in range(n) if combined[k][1] == 'x')
    u1 = r1 - nx * (nx + 1) / 2
    u2 = nx * ny - u1
    U = min(u1, u2)

    mu = nx * ny / 2
    sigma = math.sqrt(nx * ny * (nx + ny + 1) / 12)
    if sigma == 0:
        return {'U': U, 'z': 0, 'p': 1.0}

    z = (U - mu) / sigma
    p = 2 * (1 - normal_cdf(abs(z)))

    return {'U': round(U, 2), 'z': round(z, 4), 'p': round(p, 6)}


def normal_cdf(x):
    """Approximation of the standard normal CDF."""
    return 0.5 * (1 + math.erf(x / math.sqrt(2)))


# ============================================================
# MAIN
# ============================================================
def main():
    print("=" * 70)
    print("HAZARD CLASS VULNERABILITY: TOKEN PREDICTION")
    print("=" * 70)
    print()

    # Load data
    print("Loading data...")
    ctm = load_class_token_map()
    token_to_class = {k: int(v) for k, v in ctm['token_to_class'].items()}
    class_to_role = ctm['class_to_role']
    class_to_tokens = ctm.get('class_to_tokens', {})
    class_middles = ctm.get('class_to_middles', {})

    forbidden_inv = load_forbidden_inventory()
    print(f"  Forbidden transitions: {forbidden_inv['total_forbidden']}")

    morph = Morphology()

    print("Loading Currier B sequences...")
    sequences = build_b_token_sequences()
    print(f"  Lines: {len(sequences)}")
    print()

    # Extract forbidden tokens
    forbidden_tokens_list = set()
    for t in forbidden_inv['transitions']:
        forbidden_tokens_list.add(t['source'])
        forbidden_tokens_list.add(t['target'])

    # Section 1
    morph_results = section1_morph_profile(forbidden_inv, token_to_class, class_to_tokens, morph)

    # Section 2
    middle_results = section2_middle_overlap(class_to_tokens, class_to_role, class_middles, morph)

    # Section 3
    position_results = section3_position(sequences, token_to_class,
                                         morph_results['forbidden_tokens'], class_to_role)

    # Section 4
    prediction_results = section4_prediction(forbidden_tokens_list, token_to_class,
                                             class_to_tokens, class_to_role, morph)

    # ============================================================
    # SAVE RESULTS
    # ============================================================
    # Remove non-serializable data from prediction features
    clean_features = []
    for f in prediction_results.get('features', []):
        clean_features.append({
            'token': f['token'],
            'class': f['class'],
            'role': f['role'],
            'is_forbidden': f['is_forbidden'],
            'length': f['length'],
            'middle_length': f['middle_length'],
            'has_suffix': f['has_suffix'],
            'has_prefix': f['has_prefix'],
            'has_articulator': f['has_articulator'],
            'middle': f['middle'],
        })

    output = {
        'metadata': {
            'phase': 'HAZARD_CLASS_VULNERABILITY',
            'script': 'hazard_token_prediction',
            'timestamp': datetime.now().isoformat(),
            'total_forbidden_transitions': forbidden_inv['total_forbidden'],
            'total_forbidden_tokens': len(forbidden_tokens_list),
        },
        'section1_morph_profile': {
            'forbidden_tokens': morph_results['forbidden_tokens'],
            'forbidden_sources': morph_results['forbidden_sources'],
            'forbidden_targets': morph_results['forbidden_targets'],
            'profiles': morph_results['profiles'],
            'summary': morph_results['summary'],
        },
        'section2_middle_overlap': middle_results,
        'section3_position': position_results,
        'section4_prediction': {
            'n_forbidden': prediction_results['n_forbidden'],
            'n_safe': prediction_results['n_safe'],
            'feature_comparisons': prediction_results['feature_comparisons'],
            'best_discriminant': prediction_results['best_discriminant'],
            'middle_analysis': prediction_results['middle_analysis'],
            'features': clean_features,
        },
    }

    output_path = PROJECT_ROOT / 'phases' / 'HAZARD_CLASS_VULNERABILITY' / 'results' / 'hazard_token_prediction.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2)

    print(f"\nResults saved to {output_path}")


if __name__ == '__main__':
    main()
