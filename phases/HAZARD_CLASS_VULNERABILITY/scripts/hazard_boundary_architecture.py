#!/usr/bin/env python3
"""
HAZARD_CLASS_VULNERABILITY - Script 3: Hazard Boundary Architecture

Analyzes neighborhood structure around hazard classes: buffer classes,
near-miss forbidden transitions, and folio-level hazard concentration.

Sections:
  1. Forbidden transition nearest neighbors (actual occurrences + near-miss)
  2. Buffer class identification (what separates hazard zones?)
  3. Folio-level hazard concentration (density by folio/regime)
  4. Near-miss analysis (selectivity ratio of forbidden transitions)

Data dependencies:
  - class_token_map.json (CLASS_COSURVIVAL_TEST/results/)
  - phase18a_forbidden_inventory.json (phases/15-20_kernel_grammar/)
  - regime_folio_mapping.json (REGIME_SEMANTIC_INTERPRETATION/results/)
  - scripts/voynich.py (Transcript)

Constraint references:
  C109: 17 forbidden transitions
  C541: 6 hazard classes
  C554: Hazard classes cluster (dispersion 1.29, 93% of lines)
"""

import json
import sys
import math
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime

PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from scripts.voynich import Transcript

# ============================================================
# CONSTANTS
# ============================================================
HAZARD_CLASSES = {7, 8, 9, 23, 30, 31}

# ============================================================
# LOAD DATA
# ============================================================
def load_class_token_map():
    path = PROJECT_ROOT / 'phases' / 'CLASS_COSURVIVAL_TEST' / 'results' / 'class_token_map.json'
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_forbidden_inventory():
    path = PROJECT_ROOT / 'phases' / '15-20_kernel_grammar' / 'phase18a_forbidden_inventory.json'
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_regime_mapping():
    path = PROJECT_ROOT / 'phases' / 'REGIME_SEMANTIC_INTERPRETATION' / 'results' / 'regime_folio_mapping.json'
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def build_b_token_sequences():
    """Build per-folio, per-line token sequences for Currier B."""
    tx = Transcript()
    sequences = defaultdict(list)
    for token in tx.currier_b():
        key = (token.folio, token.line)
        sequences[key].append(token.word)
    return sequences


# ============================================================
# SECTION 1: FORBIDDEN TRANSITION NEAREST NEIGHBORS
# ============================================================
def section1_forbidden_neighbors(sequences, token_to_class, class_to_role, forbidden_inv):
    print("=" * 70)
    print("SECTION 1: FORBIDDEN TRANSITION NEAREST NEIGHBORS")
    print("=" * 70)
    print()

    transitions = forbidden_inv['transitions']

    # Build forbidden pair set
    forbidden_pairs = set()
    for t in transitions:
        forbidden_pairs.add((t['source'], t['target']))

    # Extract all forbidden source/target tokens
    forbidden_sources = set(t['source'] for t in transitions)
    forbidden_targets = set(t['target'] for t in transitions)

    # Scan corpus for actual occurrences and near-misses
    actual_occurrences = defaultdict(int)  # (source, target) -> count
    near_miss = defaultdict(int)  # (source, target) -> count (same classes but not forbidden)

    # Pre-compute class for each forbidden token
    source_classes = {}
    target_classes = {}
    for t in transitions:
        source_classes[t['source']] = token_to_class.get(t['source'])
        target_classes[t['target']] = token_to_class.get(t['target'])

    # Build set of forbidden class pairs
    forbidden_class_pairs = set()
    for t in transitions:
        sc = token_to_class.get(t['source'])
        tc = token_to_class.get(t['target'])
        if sc is not None and tc is not None:
            forbidden_class_pairs.add((sc, tc))

    # Track predecessor/successor classes of forbidden tokens
    predecessor_of_source = defaultdict(lambda: Counter())  # source_token -> {class: count}
    successor_of_target = defaultdict(lambda: Counter())    # target_token -> {class: count}

    for key in sorted(sequences.keys()):
        words = sequences[key]
        for i in range(len(words) - 1):
            w1, w2 = words[i], words[i + 1]
            c1 = token_to_class.get(w1)
            c2 = token_to_class.get(w2)

            # Check actual forbidden occurrence
            if (w1, w2) in forbidden_pairs:
                actual_occurrences[(w1, w2)] += 1

            # Check near-miss: same class pair as a forbidden transition,
            # but different specific token pair
            if c1 is not None and c2 is not None:
                if (c1, c2) in forbidden_class_pairs and (w1, w2) not in forbidden_pairs:
                    near_miss[(w1, w2)] += 1

            # Track predecessors of forbidden source tokens
            if w2 in forbidden_sources and c1 is not None:
                predecessor_of_source[w2][c1] += 1

            # Track successors of forbidden target tokens
            if i + 2 < len(words):
                w3 = words[i + 2]
                c3 = token_to_class.get(w3)
                if w2 in forbidden_targets and c3 is not None:
                    successor_of_target[w2][c3] += 1

    # Report actual occurrences (should be zero for truly forbidden)
    print("-" * 60)
    print("1A: Actual occurrences of forbidden transitions")
    print("-" * 60)
    total_actual = sum(actual_occurrences.values())
    print(f"\nTotal occurrences across all 17 forbidden pairs: {total_actual}")
    if total_actual > 0:
        for (s, t), count in sorted(actual_occurrences.items(), key=lambda x: -x[1]):
            print(f"  {s} -> {t}: {count}")
    else:
        print("  All 17 forbidden transitions have ZERO occurrences (confirmed forbidden)")
    print()

    # Report near-misses
    print("-" * 60)
    print("1B: Near-miss instances (same class pair, different tokens)")
    print("-" * 60)
    total_near = sum(near_miss.values())
    print(f"\nTotal near-miss transitions: {total_near}")
    print(f"Top 10 most frequent near-misses:")
    for (s, t), count in sorted(near_miss.items(), key=lambda x: -x[1])[:10]:
        c1 = token_to_class.get(s, '?')
        c2 = token_to_class.get(t, '?')
        print(f"  {s:>10} -> {t:<10} (class {c1}->{c2}): {count:5d}")
    print()

    # Report class context around forbidden source tokens
    print("-" * 60)
    print("1C: Classes preceding forbidden source tokens")
    print("-" * 60)
    for source in sorted(forbidden_sources):
        if source in predecessor_of_source:
            top_preds = predecessor_of_source[source].most_common(5)
            total_p = sum(predecessor_of_source[source].values())
            pred_str = ", ".join(f"c{c}({n})" for c, n in top_preds)
            print(f"  Before '{source}' (n={total_p}): {pred_str}")
    print()

    # Report class context after forbidden target tokens
    print("-" * 60)
    print("1D: Classes following forbidden target tokens")
    print("-" * 60)
    for target in sorted(forbidden_targets):
        if target in successor_of_target:
            top_succs = successor_of_target[target].most_common(5)
            total_s = sum(successor_of_target[target].values())
            succ_str = ", ".join(f"c{c}({n})" for c, n in top_succs)
            print(f"  After '{target}' (n={total_s}): {succ_str}")
    print()

    return {
        'actual_occurrences': total_actual,
        'actual_detail': {f"{s}->{t}": c for (s, t), c in actual_occurrences.items()},
        'near_miss_total': total_near,
        'near_miss_top10': [
            {'source': s, 'target': t, 'count': c,
             'source_class': token_to_class.get(s),
             'target_class': token_to_class.get(t)}
            for (s, t), c in sorted(near_miss.items(), key=lambda x: -x[1])[:10]
        ],
        'predecessor_summary': {
            source: dict(predecessor_of_source[source].most_common(5))
            for source in sorted(forbidden_sources) if source in predecessor_of_source
        },
        'successor_summary': {
            target: dict(successor_of_target[target].most_common(5))
            for target in sorted(forbidden_targets) if target in successor_of_target
        },
    }


# ============================================================
# SECTION 2: BUFFER CLASS IDENTIFICATION
# ============================================================
def section2_buffer_classes(sequences, token_to_class, class_to_role):
    print("=" * 70)
    print("SECTION 2: BUFFER CLASS IDENTIFICATION")
    print("=" * 70)
    print()

    # For each class, count how often it appears BETWEEN two hazard-class tokens
    # i.e., token[i-1] is hazard-class and token[i+1] is hazard-class
    buffer_counts = Counter()
    total_buffer_positions = 0

    # Also count overall class frequencies for expected rate
    overall_counts = Counter()
    total_tokens = 0

    for key in sorted(sequences.keys()):
        words = sequences[key]
        for i, word in enumerate(words):
            cls = token_to_class.get(word)
            if cls is not None:
                overall_counts[cls] += 1
                total_tokens += 1

            if i == 0 or i == len(words) - 1:
                continue

            prev_cls = token_to_class.get(words[i - 1])
            next_cls = token_to_class.get(words[i + 1])
            mid_cls = token_to_class.get(word)

            if (prev_cls is not None and next_cls is not None and mid_cls is not None
                    and prev_cls in HAZARD_CLASSES and next_cls in HAZARD_CLASSES):
                buffer_counts[mid_cls] += 1
                total_buffer_positions += 1

    print(f"Total buffer positions (between two hazard tokens): {total_buffer_positions}")
    print()

    # Chi-squared: buffer role distribution vs overall role distribution
    role_buffer = Counter()
    role_overall = Counter()
    for cls, count in buffer_counts.items():
        role = class_to_role.get(str(cls), 'UNK')
        role_buffer[role] += count
    for cls, count in overall_counts.items():
        role = class_to_role.get(str(cls), 'UNK')
        role_overall[role] += count

    print("Buffer role distribution vs overall:")
    print(f"{'Role':>25} {'Buffer':>8} {'Buffer%':>8} {'Overall':>8} {'Overall%':>9} {'Ratio':>7}")
    print("-" * 70)

    all_roles = sorted(set(list(role_buffer.keys()) + list(role_overall.keys())))
    chi2 = 0
    chi2_valid = True

    for role in all_roles:
        b_count = role_buffer.get(role, 0)
        o_count = role_overall.get(role, 0)
        b_pct = b_count / total_buffer_positions * 100 if total_buffer_positions > 0 else 0
        o_pct = o_count / total_tokens * 100 if total_tokens > 0 else 0
        ratio = (b_pct / o_pct) if o_pct > 0 else 0

        expected = total_buffer_positions * (o_count / total_tokens) if total_tokens > 0 else 0
        if expected >= 5:
            chi2 += (b_count - expected)**2 / expected
        elif expected > 0:
            chi2_valid = False

        print(f"{role:>25} {b_count:8d} {b_pct:7.1f}% {o_count:8d} {o_pct:8.1f}% {ratio:7.2f}")

    chi2_df = max(len(all_roles) - 1, 1)
    print()
    print(f"Chi-squared (buffer role vs overall role): chi2={chi2:.2f}, df={chi2_df}")
    if not chi2_valid:
        print("  WARNING: Some cells have expected < 5; use Fisher's exact for confirmation")
    print()

    # Top 5 buffer classes
    print("Top 10 buffer classes:")
    for cls, count in buffer_counts.most_common(10):
        role = class_to_role.get(str(cls), 'UNK')
        overall = overall_counts.get(cls, 0)
        ratio = count / (overall / total_tokens * total_buffer_positions) if overall > 0 else 0
        haz_tag = " [HAZ]" if cls in HAZARD_CLASSES else ""
        print(f"  Class {cls:2d} ({role[:2]}): {count:5d} buffer positions, "
              f"enrichment={ratio:.2f}x{haz_tag}")
    print()

    # Are AX classes over-represented as buffers?
    ax_classes = set(int(k) for k, v in class_to_role.items() if v == 'AUXILIARY')
    ax_buffer = sum(buffer_counts.get(c, 0) for c in ax_classes)
    ax_overall_frac = sum(overall_counts.get(c, 0) for c in ax_classes) / total_tokens if total_tokens > 0 else 0
    ax_expected = total_buffer_positions * ax_overall_frac
    ax_enrichment = ax_buffer / ax_expected if ax_expected > 0 else 0

    print(f"AX as buffers: {ax_buffer}/{total_buffer_positions} "
          f"({ax_buffer/total_buffer_positions*100:.1f}%)" if total_buffer_positions > 0 else "")
    print(f"AX overall: {ax_overall_frac*100:.1f}%")
    print(f"AX buffer enrichment: {ax_enrichment:.2f}x")
    print()

    return {
        'total_buffer_positions': total_buffer_positions,
        'buffer_counts': {str(k): v for k, v in buffer_counts.most_common()},
        'top10_buffers': [
            {'class': cls, 'role': class_to_role.get(str(cls), 'UNK'),
             'count': count, 'is_hazard': cls in HAZARD_CLASSES}
            for cls, count in buffer_counts.most_common(10)
        ],
        'role_distribution': {
            role: {
                'buffer_count': role_buffer.get(role, 0),
                'buffer_pct': round(role_buffer.get(role, 0) / max(total_buffer_positions, 1) * 100, 2),
                'overall_pct': round(role_overall.get(role, 0) / max(total_tokens, 1) * 100, 2),
            }
            for role in all_roles
        },
        'chi2': round(chi2, 2),
        'chi2_df': chi2_df,
        'ax_enrichment': round(ax_enrichment, 3),
    }


# ============================================================
# SECTION 3: FOLIO-LEVEL HAZARD CONCENTRATION
# ============================================================
def section3_folio_concentration(sequences, token_to_class, class_to_role, regime_mapping):
    print("=" * 70)
    print("SECTION 3: FOLIO-LEVEL HAZARD CONCENTRATION")
    print("=" * 70)
    print()

    # Per-folio hazard density
    folio_stats = {}
    for key in sorted(sequences.keys()):
        folio = key[0]
        words = sequences[key]
        if folio not in folio_stats:
            folio_stats[folio] = {'total': 0, 'hazard': 0, 'lines': 0}
        folio_stats[folio]['lines'] += 1
        for word in words:
            cls = token_to_class.get(word)
            if cls is not None:
                folio_stats[folio]['total'] += 1
                if cls in HAZARD_CLASSES:
                    folio_stats[folio]['hazard'] += 1

    # Compute densities
    for folio, stats in folio_stats.items():
        stats['density'] = stats['hazard'] / stats['total'] if stats['total'] > 0 else 0

    densities = [(f, s['density'], s['hazard'], s['total'])
                 for f, s in folio_stats.items()]
    densities.sort(key=lambda x: -x[1])

    print(f"Total folios: {len(folio_stats)}")
    all_densities = [s['density'] for s in folio_stats.values()]
    mean_density = sum(all_densities) / len(all_densities) if all_densities else 0
    print(f"Mean hazard density: {mean_density:.4f}")
    print()

    # Top/bottom folios
    print("Top 10 highest hazard density:")
    for folio, density, hazard, total in densities[:10]:
        print(f"  {folio:8s}: {density:.4f} ({hazard}/{total})")
    print()
    print("Bottom 10 lowest hazard density:")
    for folio, density, hazard, total in densities[-10:]:
        print(f"  {folio:8s}: {density:.4f} ({hazard}/{total})")
    print()

    # By regime
    print("-" * 60)
    print("Hazard density by REGIME")
    print("-" * 60)

    # Invert regime mapping: folio -> regime
    folio_to_regime = {}
    for regime, folios in regime_mapping.items():
        for f in folios:
            folio_to_regime[f] = regime

    regime_densities = defaultdict(list)
    for folio, stats in folio_stats.items():
        regime = folio_to_regime.get(folio, 'UNASSIGNED')
        regime_densities[regime].append(stats['density'])

    regime_stats = {}
    for regime in sorted(regime_densities.keys()):
        vals = regime_densities[regime]
        mean = sum(vals) / len(vals) if vals else 0
        std = (sum((v - mean)**2 for v in vals) / len(vals))**0.5 if len(vals) > 1 else 0
        regime_stats[regime] = {
            'n': len(vals),
            'mean': round(mean, 4),
            'std': round(std, 4),
            'min': round(min(vals), 4) if vals else 0,
            'max': round(max(vals), 4) if vals else 0,
        }
        print(f"  {regime:12s}: n={len(vals):2d}, mean={mean:.4f}, std={std:.4f}, "
              f"range=[{min(vals):.4f}, {max(vals):.4f}]")
    print()

    # Kruskal-Wallis: does regime predict hazard density?
    # Only include regimes with 3+ folios
    valid_regimes = {r: v for r, v in regime_densities.items() if len(v) >= 3 and r != 'UNASSIGNED'}
    if len(valid_regimes) >= 2:
        kw_result = kruskal_wallis(list(valid_regimes.values()))
        print(f"Kruskal-Wallis (regime predicts hazard density): H={kw_result['H']:.3f}, "
              f"df={kw_result['df']}, p={kw_result['p']:.6f}")
    else:
        kw_result = {'H': 0, 'df': 0, 'p': 1.0, 'note': 'insufficient_groups'}
        print("Kruskal-Wallis: insufficient regime groups")
    print()

    # Spearman correlations with other folio metrics
    # We'll compute hazard density vs folio token count as a simple check
    folio_densities_list = [(f, s['density']) for f, s in folio_stats.items()]
    folio_sizes = [(f, s['total']) for f, s in folio_stats.items()]

    # Sort both by folio for alignment
    folio_densities_list.sort()
    folio_sizes.sort()

    density_vals = [d for _, d in folio_densities_list]
    size_vals = [s for _, s in folio_sizes]

    if len(density_vals) >= 5:
        rho = spearman_correlation(density_vals, size_vals)
        print(f"Spearman (hazard density vs folio size): rho={rho:.4f}")
    print()

    return {
        'folio_stats': {f: {
            'hazard': s['hazard'],
            'total': s['total'],
            'density': round(s['density'], 4),
            'lines': s['lines'],
        } for f, s in folio_stats.items()},
        'mean_density': round(mean_density, 4),
        'regime_stats': regime_stats,
        'kruskal_wallis': kw_result,
        'top10_highest': [
            {'folio': f, 'density': round(d, 4), 'hazard': h, 'total': t}
            for f, d, h, t in densities[:10]
        ],
        'bottom10_lowest': [
            {'folio': f, 'density': round(d, 4), 'hazard': h, 'total': t}
            for f, d, h, t in densities[-10:]
        ],
    }


# ============================================================
# SECTION 4: NEAR-MISS ANALYSIS
# ============================================================
def section4_near_miss(sequences, token_to_class, class_to_tokens, class_to_role, forbidden_inv):
    print("=" * 70)
    print("SECTION 4: NEAR-MISS ANALYSIS (SELECTIVITY RATIO)")
    print("=" * 70)
    print()

    transitions = forbidden_inv['transitions']
    forbidden_pairs = set((t['source'], t['target']) for t in transitions)

    # Count all token-pair transitions between hazard-class tokens
    hazard_token_transitions = Counter()  # (token1, token2) -> count
    hazard_class_transitions = Counter()  # (class1, class2) -> count

    for key in sorted(sequences.keys()):
        words = sequences[key]
        for i in range(len(words) - 1):
            w1, w2 = words[i], words[i + 1]
            c1 = token_to_class.get(w1)
            c2 = token_to_class.get(w2)
            if c1 is not None and c2 is not None:
                if c1 in HAZARD_CLASSES and c2 in HAZARD_CLASSES:
                    hazard_token_transitions[(w1, w2)] += 1
                    hazard_class_transitions[(c1, c2)] += 1

    # Total possible token pairs within hazard classes
    hazard_tokens = set()
    for cls in HAZARD_CLASSES:
        for t in class_to_tokens.get(str(cls), []):
            hazard_tokens.add(t)

    total_possible = len(hazard_tokens) * len(hazard_tokens)  # including self-transitions
    total_observed = len(hazard_token_transitions)
    total_forbidden = len(forbidden_pairs)

    print(f"Hazard-class tokens: {len(hazard_tokens)}")
    print(f"Possible token pairs (N x N): {total_possible}")
    print(f"Observed unique pairs: {total_observed}")
    print(f"Forbidden pairs: {total_forbidden}")
    print()

    # Selectivity ratio: what fraction of possible transitions are forbidden?
    selectivity_possible = total_forbidden / total_possible if total_possible > 0 else 0
    selectivity_observed = total_forbidden / total_observed if total_observed > 0 else 0

    print(f"Selectivity (forbidden / possible): {selectivity_possible:.6f} "
          f"({total_forbidden}/{total_possible})")
    print(f"Selectivity (forbidden / observed): {selectivity_observed:.4f} "
          f"({total_forbidden}/{total_observed})")
    print()

    # Most frequent non-forbidden transitions between hazard classes
    non_forbidden = {k: v for k, v in hazard_token_transitions.items()
                     if k not in forbidden_pairs}

    print("Top 15 most frequent NON-forbidden hazard-class transitions:")
    for (w1, w2), count in sorted(non_forbidden.items(), key=lambda x: -x[1])[:15]:
        c1 = token_to_class.get(w1, '?')
        c2 = token_to_class.get(w2, '?')
        print(f"  {w1:>10} -> {w2:<10} (c{c1}->c{c2}): {count:5d}")
    print()

    # Class-level transition distribution
    print("Hazard class-class transition counts:")
    for (c1, c2), count in sorted(hazard_class_transitions.items(), key=lambda x: -x[1]):
        print(f"  c{c1} -> c{c2}: {count:5d}")
    print()

    # Which transitions are forbidden vs allowed within each class pair?
    print("-" * 60)
    print("Forbidden vs allowed within each hazard class pair:")
    print("-" * 60)
    for (c1, c2) in sorted(set(hazard_class_transitions.keys())):
        # Count forbidden pairs in this class pair
        forb_in_pair = [(s, t) for (s, t) in forbidden_pairs
                        if token_to_class.get(s) == c1 and token_to_class.get(t) == c2]
        # Count observed token pairs in this class pair
        obs_in_pair = {(w1, w2): cnt for (w1, w2), cnt in hazard_token_transitions.items()
                       if token_to_class.get(w1) == c1 and token_to_class.get(w2) == c2}
        total_vol = sum(obs_in_pair.values())
        n_obs = len(obs_in_pair)
        n_forb = len(forb_in_pair)

        # Total possible for this class pair
        c1_members = class_to_tokens.get(str(c1), [])
        c2_members = class_to_tokens.get(str(c2), [])
        n_possible = len(c1_members) * len(c2_members)

        print(f"  c{c1}->c{c2}: {n_forb} forbidden / {n_obs} observed / {n_possible} possible, "
              f"volume={total_vol}")
        if forb_in_pair:
            for s, t in forb_in_pair:
                print(f"    FORBIDDEN: {s} -> {t}")
    print()

    return {
        'hazard_tokens_count': len(hazard_tokens),
        'total_possible_pairs': total_possible,
        'total_observed_pairs': total_observed,
        'total_forbidden': total_forbidden,
        'selectivity_over_possible': round(selectivity_possible, 6),
        'selectivity_over_observed': round(selectivity_observed, 4),
        'top15_non_forbidden': [
            {'source': w1, 'target': w2, 'count': c,
             'source_class': token_to_class.get(w1),
             'target_class': token_to_class.get(w2)}
            for (w1, w2), c in sorted(non_forbidden.items(), key=lambda x: -x[1])[:15]
        ],
        'class_pair_transitions': {
            f"{c1}->{c2}": count
            for (c1, c2), count in sorted(hazard_class_transitions.items(), key=lambda x: -x[1])
        },
    }


# ============================================================
# STATISTICAL UTILITIES
# ============================================================
def kruskal_wallis(groups):
    """Kruskal-Wallis H test (non-parametric ANOVA)."""
    # Combine all values with group labels
    all_values = []
    for i, group in enumerate(groups):
        for v in group:
            all_values.append((v, i))

    # Sort and rank
    all_values.sort(key=lambda x: x[0])
    n = len(all_values)
    ranks = [0.0] * n

    i = 0
    while i < n:
        j = i
        while j < n and all_values[j][0] == all_values[i][0]:
            j += 1
        avg_rank = (i + j + 1) / 2
        for k in range(i, j):
            ranks[k] = avg_rank
        i = j

    # Sum of ranks per group
    k = len(groups)
    group_rank_sums = [0.0] * k
    group_sizes = [len(g) for g in groups]

    for idx in range(n):
        group_idx = all_values[idx][1]
        group_rank_sums[group_idx] += ranks[idx]

    # H statistic
    H = 0
    for i in range(k):
        ni = group_sizes[i]
        if ni > 0:
            H += (group_rank_sums[i] ** 2) / ni
    H = (12 / (n * (n + 1))) * H - 3 * (n + 1)

    df = k - 1

    # Chi-squared approximation for p-value
    p = chi2_sf(H, df)

    return {'H': round(H, 4), 'df': df, 'p': round(p, 6)}


def chi2_sf(x, df):
    """Survival function (1 - CDF) of chi-squared distribution, approximation."""
    if df <= 0 or x <= 0:
        return 1.0
    # Use normal approximation for chi-squared
    z = ((x / df) ** (1/3) - (1 - 2 / (9 * df))) / math.sqrt(2 / (9 * df))
    return 1 - normal_cdf(z)


def normal_cdf(x):
    """Standard normal CDF approximation."""
    return 0.5 * (1 + math.erf(x / math.sqrt(2)))


def spearman_correlation(x, y):
    """Spearman rank correlation coefficient."""
    n = len(x)
    if n < 3:
        return 0.0

    # Rank both
    def rank_data(data):
        sorted_idx = sorted(range(n), key=lambda i: data[i])
        ranks = [0.0] * n
        i = 0
        while i < n:
            j = i
            while j < n and data[sorted_idx[j]] == data[sorted_idx[i]]:
                j += 1
            avg_rank = (i + j + 1) / 2
            for k in range(i, j):
                ranks[sorted_idx[k]] = avg_rank
            i = j
        return ranks

    rx = rank_data(x)
    ry = rank_data(y)

    # Pearson on ranks
    mean_rx = sum(rx) / n
    mean_ry = sum(ry) / n

    num = sum((rx[i] - mean_rx) * (ry[i] - mean_ry) for i in range(n))
    den_x = sum((rx[i] - mean_rx) ** 2 for i in range(n))
    den_y = sum((ry[i] - mean_ry) ** 2 for i in range(n))

    if den_x == 0 or den_y == 0:
        return 0.0

    return num / math.sqrt(den_x * den_y)


# ============================================================
# MAIN
# ============================================================
def main():
    print("=" * 70)
    print("HAZARD CLASS VULNERABILITY: BOUNDARY ARCHITECTURE")
    print("=" * 70)
    print()

    # Load data
    print("Loading data...")
    ctm = load_class_token_map()
    token_to_class = {k: int(v) for k, v in ctm['token_to_class'].items()}
    class_to_role = ctm['class_to_role']
    class_to_tokens = ctm.get('class_to_tokens', {})

    forbidden_inv = load_forbidden_inventory()
    regime_mapping = load_regime_mapping()

    print(f"  Forbidden transitions: {forbidden_inv['total_forbidden']}")
    print(f"  Regimes: {len(regime_mapping)}")

    print("Loading Currier B sequences...")
    sequences = build_b_token_sequences()
    total_tokens = sum(len(v) for v in sequences.values())
    print(f"  Lines: {len(sequences)}")
    print(f"  Total tokens: {total_tokens}")
    print()

    # Section 1
    neighbor_results = section1_forbidden_neighbors(
        sequences, token_to_class, class_to_role, forbidden_inv)

    # Section 2
    buffer_results = section2_buffer_classes(
        sequences, token_to_class, class_to_role)

    # Section 3
    folio_results = section3_folio_concentration(
        sequences, token_to_class, class_to_role, regime_mapping)

    # Section 4
    nearmiss_results = section4_near_miss(
        sequences, token_to_class, class_to_tokens, class_to_role, forbidden_inv)

    # ============================================================
    # SAVE RESULTS
    # ============================================================
    output = {
        'metadata': {
            'phase': 'HAZARD_CLASS_VULNERABILITY',
            'script': 'hazard_boundary_architecture',
            'timestamp': datetime.now().isoformat(),
            'hazard_classes': sorted(HAZARD_CLASSES),
            'total_forbidden': forbidden_inv['total_forbidden'],
            'total_b_tokens': total_tokens,
        },
        'section1_forbidden_neighbors': neighbor_results,
        'section2_buffer_classes': buffer_results,
        'section3_folio_concentration': folio_results,
        'section4_near_miss': nearmiss_results,
    }

    output_path = PROJECT_ROOT / 'phases' / 'HAZARD_CLASS_VULNERABILITY' / 'results' / 'hazard_boundary_architecture.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2)

    print(f"\nResults saved to {output_path}")


if __name__ == '__main__':
    main()
